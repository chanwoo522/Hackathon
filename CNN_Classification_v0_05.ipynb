{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "CNN Classification v0.04.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM2NfKPH0x7bg3NVt9mNalL",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chanwoo522/Hackathon/blob/main/CNN_Classification_v0_05.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_7ExL9mBFv8"
      },
      "source": [
        "## 라이브러리 임포트"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3EJ9vJD8RaD"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torchvision import transforms, datasets, utils\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "import re\n",
        "import shutil"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OPIaATPUIzYU"
      },
      "source": [
        "USE_CUDA = torch.cuda.is_available()\n",
        "DEVICE = torch.device(\"cuda\" if USE_CUDA else \"cpu\")"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YjFu_3-tBSZ3"
      },
      "source": [
        "## 구글 드라이브 마운트"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pK5FN0GbN1qj",
        "outputId": "ac02c89a-b2c6-486f-d53f-307ca40138bb"
      },
      "source": [
        "# load image files\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "boFZ0eN1BQeI"
      },
      "source": [
        "## 작업 폴더 설정"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ocIf9WqpDssw"
      },
      "source": [
        "## directory 설정\n",
        "cur_dir = os.path.abspath('/content/drive/Shareddrives/aircraft')\n",
        "image_dir = os.path.join(cur_dir, 'edge')\n",
        "image_files = [fname for fname in os.listdir(image_dir) if os.path.splitext(fname)[-1] == '.png']"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OwbgFf6Z8Xoi",
        "outputId": "1dca3839-ada9-4492-b8a7-97421e16775e"
      },
      "source": [
        "# labeling\n",
        "\n",
        "Labels = set()\n",
        "\n",
        "for image_file in image_files:\n",
        "    file_name = os.path.splitext(image_file)[0]\n",
        "    class_name = re.sub('_\\d+', '', file_name)\n",
        "    Labels.add(class_name)\n",
        "Labels = list(Labels)\n",
        "\n",
        "# ['j10','j11','j15','j16','j20','j31','JL10','j6',\n",
        "#  'y8g','y9jb','y20','kj2000','bjk005','ch3','wingloong',\n",
        "#  'xianglong','z9','z18','mig31','su24','su27','su30',\n",
        "#  'su35','su57','tu95ms','tu142','a50','il38','il20',\n",
        "#  'f2','e767','ec1','ch47j','p1','f4','f5','fa50','f15',\n",
        "#  'f16','fa18','f22','f35','a10','b1','b2','c130','p3',\n",
        "#  'p8','rc135','e737','kc330','u2v']\n",
        "print(Labels)\n",
        "img_size = 512"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['f-35 lightning', 'b-1 lancer']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EbGrBPR3bnEr"
      },
      "source": [
        "## Experiment directory setting\n",
        "\n",
        "train_dir = os.path.join(cur_dir, 'train_dir')\n",
        "test_dir = os.path.join(cur_dir, 'test_dir')\n",
        "\n",
        "os.makedirs(train_dir, exist_ok=True)\n",
        "os.makedirs(test_dir, exist_ok=True)\n",
        "\n",
        "for label in Labels:\n",
        "  label_dir = os.path.join(train_dir, label)\n",
        "  os.makedirs(label_dir, exist_ok=True)\n",
        "  label_dir = os.path.join(test_dir, label)\n",
        "  os.makedirs(label_dir, exist_ok=True)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RdjznjFJCIHo"
      },
      "source": [
        "## 이미지 파일 train, test data로 분리"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-axG8zDBp52"
      },
      "source": [
        "# practice file separate\n",
        "cnt = 0\n",
        "previous_class = \"\"\n",
        "image_files.sort()\n",
        "\n",
        "for image_file in image_files:\n",
        "  file_name = os.path.splitext(image_file)[0]\n",
        "  class_name = re.sub('_\\d+', '', file_name)\n",
        "  if class_name == previous_class:\n",
        "    cnt += 1\n",
        "  else:\n",
        "    cnt = 1\n",
        "  if cnt <= 200:\n",
        "    for label in Labels:\n",
        "        if label == class_name:\n",
        "          cpath = os.path.join(train_dir, label)\n",
        "          image_path = os.path.join(image_dir, image_file)\n",
        "          shutil.copy(image_path, cpath)\n",
        "        else:\n",
        "          pass\n",
        "  else:\n",
        "    for label in Labels:\n",
        "        if label == class_name:\n",
        "          cpath = os.path.join(test_dir, label)\n",
        "          image_path = os.path.join(image_dir, image_file)\n",
        "          shutil.copy(image_path, cpath)\n",
        "        else:\n",
        "          pass\n",
        "  previous_class = class_name"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZ0tmzvMCYuG"
      },
      "source": [
        "# Data load and transform\n",
        "transform0 = transforms.Compose([\n",
        "                                transforms.Resize((512,512)), \n",
        "                                transforms.RandomHorizontalFlip(),\n",
        "                                transforms.ToTensor(),\n",
        "                                transforms.Normalize((0.5,0.5,0.5),\n",
        "                                                     (0.5,0.5,0.5))\n",
        "])\n",
        "\n",
        "transform1 = transforms.Compose([\n",
        "                                transforms.RandomCrop(224),\n",
        "                                transforms.RandomHorizontalFlip(),\n",
        "                                transforms.ToTensor(),\n",
        "                                transforms.Normalize((0.5,0.5,0.5),\n",
        "                                                     (0.5,0.5,0.5))\n",
        "])\n",
        "transform2 = transforms.Compose([\n",
        "                                transforms.RandomCrop(32, padding=4),\n",
        "                                transforms.RandomHorizontalFlip(),\n",
        "                                transforms.ToTensor(),\n",
        "                                transforms.Normalize((0.5,0.5,0.5),\n",
        "                                                     (0.5,0.5,0.5))\n",
        "])\n",
        "train_dataset = datasets.ImageFolder(root=train_dir, transform=transform0)\n",
        "test_dataset = datasets.ImageFolder(root=test_dir, transform=transform0)\n",
        "\n",
        "# Hyperparameter\n",
        "\n",
        "\"\"\"\n",
        "추가로 실험해봐야 할 부분\n",
        "1. epoch / batch size / lr / stepsize 조정\n",
        "2. crop 조정\n",
        "\"\"\"\n",
        "\n",
        "EPOCHS = 50       # 40, 150, 300\n",
        "BATCH_SIZE = 8   # 16, 64, 128\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    dataset = train_dataset,\n",
        "    batch_size = BATCH_SIZE\n",
        ")\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    dataset = test_dataset,\n",
        "    batch_size = BATCH_SIZE\n",
        ")"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "Uo0OEsxpuAnr",
        "outputId": "409e97bf-e391-4b19-8fe5-e61cd4b4d381"
      },
      "source": [
        "dataiter = iter(train_loader)\n",
        "images, labels = next(dataiter)\n",
        "img = utils.make_grid(images, padding=0)\n",
        "npimg = img.numpy()\n",
        "plt.figure(figsize = (10,7))\n",
        "plt.imshow(np.transpose(npimg,(1,2,0)))\n",
        "plt.show()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x504 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8VEjbj5_Uk5y",
        "outputId": "b0ea1dc2-d512-4423-d3ea-7f658b6c660c"
      },
      "source": [
        "img.shape"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 512, 4096])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5vE9HvQZm_8r",
        "outputId": "ec772f26-14ea-4d3c-9c0a-01edcb1b5028"
      },
      "source": [
        "print(len(Labels))\n",
        "\"\"\"\n",
        "다른 데이터셋으로 실험 시 반드시 num_classes를 label의 수로 변경\n",
        "이미지 사이즈 조정 시 출력계층 고치기 self.linear = nn.Linear(64*49, num_classes)\n",
        "\"\"\"\n",
        "Labels"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['f-35 lightning', 'b-1 lancer']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CTlmqBMN7ksj"
      },
      "source": [
        "class BasicBlock(nn.Module):\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3,\n",
        "                               stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, planes,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, num_classes=2):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_planes = 16\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(16)\n",
        "        self.layer1 = self._make_layer(16, 2, stride=1)\n",
        "        self.layer2 = self._make_layer(32, 2, stride=2)\n",
        "        self.layer3 = self._make_layer(64, 2, stride=2)\n",
        "        self.linear = nn.Linear(64*256, num_classes)\n",
        "\n",
        "    def _make_layer(self, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(BasicBlock(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = F.avg_pool2d(out, 8)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C0SUiS5yyJI_"
      },
      "source": [
        "model = ResNet().to(DEVICE)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1,\n",
        "                      momentum=0.9, weight_decay=0.005)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SrODV4UeJ6uZ",
        "outputId": "daa5abec-db0e-4b2e-c38a-2826995c9546"
      },
      "source": [
        "print(model)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ResNet(\n",
            "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "  (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (layer1): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential(\n",
            "        (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential(\n",
            "        (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "  )\n",
            "  (linear): Linear(in_features=16384, out_features=2, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FU17_tx8Nfqc"
      },
      "source": [
        "def train(model, train_loader, optimizer, epoch):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(DEVICE), target.to(DEVICE)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.cross_entropy(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Exbxq_vAYqoc"
      },
      "source": [
        "def evaluate(model, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(DEVICE), target.to(DEVICE)\n",
        "            output = model(data)\n",
        "\n",
        "            # 배치 오차를 합산\n",
        "            test_loss += F.cross_entropy(output, target,\n",
        "                                         reduction='mean').item()\n",
        "\n",
        "            # 가장 높은 값을 가진 인덱스가 바로 예측값\n",
        "            pred = output.max(1, keepdim=True)[1]\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    test_accuracy = 100. * correct / len(test_loader.dataset)\n",
        "    return test_loss, test_accuracy"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQ_PTUoYUHjm"
      },
      "source": [
        "import gc\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-UT6KQyBE5zm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4554bc74-a493-49c1-fb3a-50b3c65e0615"
      },
      "source": [
        "for epoch in range(1, EPOCHS + 1):\n",
        "\n",
        "    train(model, train_loader, optimizer, epoch)\n",
        "    test_loss, test_accuracy = evaluate(model, test_loader)\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "    print('[{}] Test Loss: {:.4f}, Accuracy: {:.2f}%'.format(\n",
        "          epoch, test_loss, test_accuracy))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1] Test Loss: 1827724.0825, Accuracy: 13.00%\n",
            "[2] Test Loss: 2309450.7200, Accuracy: 13.00%\n",
            "[3] Test Loss: 1990018169.6000, Accuracy: 13.00%\n",
            "[4] Test Loss: 19542045655.0400, Accuracy: 13.00%\n",
            "[5] Test Loss: 203643990090711.0312, Accuracy: 13.00%\n",
            "[6] Test Loss: 7329910163316408.0000, Accuracy: 13.00%\n",
            "[7] Test Loss: 1696265677241046784.0000, Accuracy: 13.00%\n",
            "[8] Test Loss: nan, Accuracy: 87.00%\n",
            "[9] Test Loss: nan, Accuracy: 87.00%\n",
            "[10] Test Loss: nan, Accuracy: 87.00%\n",
            "[11] Test Loss: nan, Accuracy: 87.00%\n",
            "[12] Test Loss: nan, Accuracy: 87.00%\n",
            "[13] Test Loss: nan, Accuracy: 87.00%\n",
            "[14] Test Loss: nan, Accuracy: 87.00%\n",
            "[15] Test Loss: nan, Accuracy: 87.00%\n",
            "[16] Test Loss: nan, Accuracy: 87.00%\n",
            "[17] Test Loss: nan, Accuracy: 87.00%\n",
            "[18] Test Loss: nan, Accuracy: 87.00%\n",
            "[19] Test Loss: nan, Accuracy: 87.00%\n",
            "[20] Test Loss: nan, Accuracy: 87.00%\n",
            "[21] Test Loss: nan, Accuracy: 87.00%\n",
            "[22] Test Loss: nan, Accuracy: 87.00%\n",
            "[23] Test Loss: nan, Accuracy: 87.00%\n",
            "[24] Test Loss: nan, Accuracy: 87.00%\n",
            "[25] Test Loss: nan, Accuracy: 87.00%\n",
            "[26] Test Loss: nan, Accuracy: 87.00%\n",
            "[27] Test Loss: nan, Accuracy: 87.00%\n",
            "[28] Test Loss: nan, Accuracy: 87.00%\n",
            "[29] Test Loss: nan, Accuracy: 87.00%\n",
            "[30] Test Loss: nan, Accuracy: 87.00%\n",
            "[31] Test Loss: nan, Accuracy: 87.00%\n",
            "[32] Test Loss: nan, Accuracy: 87.00%\n",
            "[33] Test Loss: nan, Accuracy: 87.00%\n",
            "[34] Test Loss: nan, Accuracy: 87.00%\n",
            "[35] Test Loss: nan, Accuracy: 87.00%\n",
            "[36] Test Loss: nan, Accuracy: 87.00%\n",
            "[37] Test Loss: nan, Accuracy: 87.00%\n",
            "[38] Test Loss: nan, Accuracy: 87.00%\n",
            "[39] Test Loss: nan, Accuracy: 87.00%\n",
            "[40] Test Loss: nan, Accuracy: 87.00%\n",
            "[41] Test Loss: nan, Accuracy: 87.00%\n",
            "[42] Test Loss: nan, Accuracy: 87.00%\n",
            "[43] Test Loss: nan, Accuracy: 87.00%\n",
            "[44] Test Loss: nan, Accuracy: 87.00%\n",
            "[45] Test Loss: nan, Accuracy: 87.00%\n",
            "[46] Test Loss: nan, Accuracy: 87.00%\n",
            "[47] Test Loss: nan, Accuracy: 87.00%\n",
            "[48] Test Loss: nan, Accuracy: 87.00%\n",
            "[49] Test Loss: nan, Accuracy: 87.00%\n",
            "[50] Test Loss: nan, Accuracy: 87.00%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6IWBtfOqkNh"
      },
      "source": [
        "## 신경망 검사"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GosNa7uWp3Qf"
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "\n",
        "dataiter = iter(test_loader)\n",
        "images, labels = dataiter.next()"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5RUa3GwuiXO"
      },
      "source": [
        "## 가중치, 파라미터 확인"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DTTiSc4ezc9r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab9db8be-cbb0-4c99-aef4-7aa90da56c4a"
      },
      "source": [
        "print(\"Model's state dict:\")\n",
        "for param_tensor in model.state_dict():\n",
        "  print(param_tensor, '\\t', model.state_dict()[param_tensor].size())"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model's state dict:\n",
            "conv1.weight \t torch.Size([16, 3, 3, 3])\n",
            "bn1.weight \t torch.Size([16])\n",
            "bn1.bias \t torch.Size([16])\n",
            "bn1.running_mean \t torch.Size([16])\n",
            "bn1.running_var \t torch.Size([16])\n",
            "bn1.num_batches_tracked \t torch.Size([])\n",
            "layer1.0.conv1.weight \t torch.Size([16, 16, 3, 3])\n",
            "layer1.0.bn1.weight \t torch.Size([16])\n",
            "layer1.0.bn1.bias \t torch.Size([16])\n",
            "layer1.0.bn1.running_mean \t torch.Size([16])\n",
            "layer1.0.bn1.running_var \t torch.Size([16])\n",
            "layer1.0.bn1.num_batches_tracked \t torch.Size([])\n",
            "layer1.0.conv2.weight \t torch.Size([16, 16, 3, 3])\n",
            "layer1.0.bn2.weight \t torch.Size([16])\n",
            "layer1.0.bn2.bias \t torch.Size([16])\n",
            "layer1.0.bn2.running_mean \t torch.Size([16])\n",
            "layer1.0.bn2.running_var \t torch.Size([16])\n",
            "layer1.0.bn2.num_batches_tracked \t torch.Size([])\n",
            "layer1.1.conv1.weight \t torch.Size([16, 16, 3, 3])\n",
            "layer1.1.bn1.weight \t torch.Size([16])\n",
            "layer1.1.bn1.bias \t torch.Size([16])\n",
            "layer1.1.bn1.running_mean \t torch.Size([16])\n",
            "layer1.1.bn1.running_var \t torch.Size([16])\n",
            "layer1.1.bn1.num_batches_tracked \t torch.Size([])\n",
            "layer1.1.conv2.weight \t torch.Size([16, 16, 3, 3])\n",
            "layer1.1.bn2.weight \t torch.Size([16])\n",
            "layer1.1.bn2.bias \t torch.Size([16])\n",
            "layer1.1.bn2.running_mean \t torch.Size([16])\n",
            "layer1.1.bn2.running_var \t torch.Size([16])\n",
            "layer1.1.bn2.num_batches_tracked \t torch.Size([])\n",
            "layer2.0.conv1.weight \t torch.Size([32, 16, 3, 3])\n",
            "layer2.0.bn1.weight \t torch.Size([32])\n",
            "layer2.0.bn1.bias \t torch.Size([32])\n",
            "layer2.0.bn1.running_mean \t torch.Size([32])\n",
            "layer2.0.bn1.running_var \t torch.Size([32])\n",
            "layer2.0.bn1.num_batches_tracked \t torch.Size([])\n",
            "layer2.0.conv2.weight \t torch.Size([32, 32, 3, 3])\n",
            "layer2.0.bn2.weight \t torch.Size([32])\n",
            "layer2.0.bn2.bias \t torch.Size([32])\n",
            "layer2.0.bn2.running_mean \t torch.Size([32])\n",
            "layer2.0.bn2.running_var \t torch.Size([32])\n",
            "layer2.0.bn2.num_batches_tracked \t torch.Size([])\n",
            "layer2.0.shortcut.0.weight \t torch.Size([32, 16, 1, 1])\n",
            "layer2.0.shortcut.1.weight \t torch.Size([32])\n",
            "layer2.0.shortcut.1.bias \t torch.Size([32])\n",
            "layer2.0.shortcut.1.running_mean \t torch.Size([32])\n",
            "layer2.0.shortcut.1.running_var \t torch.Size([32])\n",
            "layer2.0.shortcut.1.num_batches_tracked \t torch.Size([])\n",
            "layer2.1.conv1.weight \t torch.Size([32, 32, 3, 3])\n",
            "layer2.1.bn1.weight \t torch.Size([32])\n",
            "layer2.1.bn1.bias \t torch.Size([32])\n",
            "layer2.1.bn1.running_mean \t torch.Size([32])\n",
            "layer2.1.bn1.running_var \t torch.Size([32])\n",
            "layer2.1.bn1.num_batches_tracked \t torch.Size([])\n",
            "layer2.1.conv2.weight \t torch.Size([32, 32, 3, 3])\n",
            "layer2.1.bn2.weight \t torch.Size([32])\n",
            "layer2.1.bn2.bias \t torch.Size([32])\n",
            "layer2.1.bn2.running_mean \t torch.Size([32])\n",
            "layer2.1.bn2.running_var \t torch.Size([32])\n",
            "layer2.1.bn2.num_batches_tracked \t torch.Size([])\n",
            "layer3.0.conv1.weight \t torch.Size([64, 32, 3, 3])\n",
            "layer3.0.bn1.weight \t torch.Size([64])\n",
            "layer3.0.bn1.bias \t torch.Size([64])\n",
            "layer3.0.bn1.running_mean \t torch.Size([64])\n",
            "layer3.0.bn1.running_var \t torch.Size([64])\n",
            "layer3.0.bn1.num_batches_tracked \t torch.Size([])\n",
            "layer3.0.conv2.weight \t torch.Size([64, 64, 3, 3])\n",
            "layer3.0.bn2.weight \t torch.Size([64])\n",
            "layer3.0.bn2.bias \t torch.Size([64])\n",
            "layer3.0.bn2.running_mean \t torch.Size([64])\n",
            "layer3.0.bn2.running_var \t torch.Size([64])\n",
            "layer3.0.bn2.num_batches_tracked \t torch.Size([])\n",
            "layer3.0.shortcut.0.weight \t torch.Size([64, 32, 1, 1])\n",
            "layer3.0.shortcut.1.weight \t torch.Size([64])\n",
            "layer3.0.shortcut.1.bias \t torch.Size([64])\n",
            "layer3.0.shortcut.1.running_mean \t torch.Size([64])\n",
            "layer3.0.shortcut.1.running_var \t torch.Size([64])\n",
            "layer3.0.shortcut.1.num_batches_tracked \t torch.Size([])\n",
            "layer3.1.conv1.weight \t torch.Size([64, 64, 3, 3])\n",
            "layer3.1.bn1.weight \t torch.Size([64])\n",
            "layer3.1.bn1.bias \t torch.Size([64])\n",
            "layer3.1.bn1.running_mean \t torch.Size([64])\n",
            "layer3.1.bn1.running_var \t torch.Size([64])\n",
            "layer3.1.bn1.num_batches_tracked \t torch.Size([])\n",
            "layer3.1.conv2.weight \t torch.Size([64, 64, 3, 3])\n",
            "layer3.1.bn2.weight \t torch.Size([64])\n",
            "layer3.1.bn2.bias \t torch.Size([64])\n",
            "layer3.1.bn2.running_mean \t torch.Size([64])\n",
            "layer3.1.bn2.running_var \t torch.Size([64])\n",
            "layer3.1.bn2.num_batches_tracked \t torch.Size([])\n",
            "linear.weight \t torch.Size([2, 16384])\n",
            "linear.bias \t torch.Size([2])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uootomSzl6FN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bade6361-f1e9-41dc-d092-fb27708ed93b"
      },
      "source": [
        "print(\"Optimizer's state_dict:\")\n",
        "for var_name in optimizer.state_dict():\n",
        "  print(var_name, '\\t', optimizer.state_dict()[var_name])"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Optimizer's state_dict:\n",
            "state \t {0: {'momentum_buffer': tensor([[[[        nan,         nan,         nan],\n",
            "          [        nan,         nan,         nan],\n",
            "          [        nan,         nan,         nan]],\n",
            "\n",
            "         [[        nan,         nan,         nan],\n",
            "          [        nan,         nan,         nan],\n",
            "          [        nan,         nan,         nan]],\n",
            "\n",
            "         [[        nan,         nan,         nan],\n",
            "          [        nan,         nan,         nan],\n",
            "          [        nan,         nan,         nan]]],\n",
            "\n",
            "\n",
            "        [[[        nan,         nan,         nan],\n",
            "          [        nan,         nan,         nan],\n",
            "          [        nan,         nan,         nan]],\n",
            "\n",
            "         [[        nan,         nan,         nan],\n",
            "          [        nan,         nan,         nan],\n",
            "          [        nan,         nan,         nan]],\n",
            "\n",
            "         [[        nan,         nan,         nan],\n",
            "          [        nan,         nan,         nan],\n",
            "          [        nan,         nan,         nan]]],\n",
            "\n",
            "\n",
            "        [[[ 4.7519e-01,  3.5220e-01,  1.8584e-01],\n",
            "          [ 3.5770e-01,  2.9214e-01,  1.6258e-01],\n",
            "          [ 3.5671e-01,  3.4222e-01,  2.4718e-01]],\n",
            "\n",
            "         [[ 4.7484e-01,  3.5203e-01,  1.8633e-01],\n",
            "          [ 3.5772e-01,  2.9227e-01,  1.6240e-01],\n",
            "          [ 3.5647e-01,  3.4298e-01,  2.4712e-01]],\n",
            "\n",
            "         [[ 4.7485e-01,  3.5220e-01,  1.8663e-01],\n",
            "          [ 3.5765e-01,  2.9218e-01,  1.6239e-01],\n",
            "          [ 3.5601e-01,  3.4284e-01,  2.4723e-01]]],\n",
            "\n",
            "\n",
            "        [[[-4.1611e-01, -2.8282e-01, -3.6824e-01],\n",
            "          [-4.8856e-01, -3.5399e-01, -4.4789e-01],\n",
            "          [-6.5615e-01, -5.1807e-01, -6.0833e-01]],\n",
            "\n",
            "         [[-4.1657e-01, -2.8255e-01, -3.6839e-01],\n",
            "          [-4.8870e-01, -3.5445e-01, -4.4774e-01],\n",
            "          [-6.5617e-01, -5.1784e-01, -6.0909e-01]],\n",
            "\n",
            "         [[-4.1614e-01, -2.8294e-01, -3.6900e-01],\n",
            "          [-4.8824e-01, -3.5383e-01, -4.4788e-01],\n",
            "          [-6.5649e-01, -5.1809e-01, -6.0827e-01]]],\n",
            "\n",
            "\n",
            "        [[[-3.4510e+00, -3.2580e+00, -3.5806e+00],\n",
            "          [-4.2025e+00, -4.0250e+00, -4.3853e+00],\n",
            "          [-1.9098e+00, -1.6248e+00, -1.9571e+00]],\n",
            "\n",
            "         [[-3.4508e+00, -3.2584e+00, -3.5803e+00],\n",
            "          [-4.2026e+00, -4.0247e+00, -4.3853e+00],\n",
            "          [-1.9101e+00, -1.6239e+00, -1.9576e+00]],\n",
            "\n",
            "         [[-3.4508e+00, -3.2586e+00, -3.5798e+00],\n",
            "          [-4.2029e+00, -4.0246e+00, -4.3846e+00],\n",
            "          [-1.9099e+00, -1.6239e+00, -1.9571e+00]]],\n",
            "\n",
            "\n",
            "        [[[ 6.8596e+01,  6.6710e+01,  6.3887e+01],\n",
            "          [ 5.5518e+01,  5.4078e+01,  5.1582e+01],\n",
            "          [ 2.8269e+01,  2.7146e+01,  2.5137e+01]],\n",
            "\n",
            "         [[ 6.8596e+01,  6.6709e+01,  6.3887e+01],\n",
            "          [ 5.5518e+01,  5.4078e+01,  5.1582e+01],\n",
            "          [ 2.8269e+01,  2.7145e+01,  2.5137e+01]],\n",
            "\n",
            "         [[ 6.8596e+01,  6.6709e+01,  6.3887e+01],\n",
            "          [ 5.5518e+01,  5.4078e+01,  5.1583e+01],\n",
            "          [ 2.8269e+01,  2.7146e+01,  2.5137e+01]]],\n",
            "\n",
            "\n",
            "        [[[ 1.3676e-01,  1.4694e-01,  1.4190e-01],\n",
            "          [ 7.9404e-02,  9.8984e-02,  1.0064e-01],\n",
            "          [ 1.5981e-02,  4.8432e-02,  6.2656e-02]],\n",
            "\n",
            "         [[ 1.3722e-01,  1.4748e-01,  1.4241e-01],\n",
            "          [ 8.0080e-02,  9.8885e-02,  1.0069e-01],\n",
            "          [ 1.5205e-02,  4.8161e-02,  6.2570e-02]],\n",
            "\n",
            "         [[ 1.3717e-01,  1.4728e-01,  1.4198e-01],\n",
            "          [ 7.9972e-02,  9.9638e-02,  1.0118e-01],\n",
            "          [ 1.5738e-02,  4.8019e-02,  6.2213e-02]]],\n",
            "\n",
            "\n",
            "        [[[-9.8636e-03, -1.5190e-02, -1.8489e-02],\n",
            "          [ 6.3204e-04, -4.5083e-03, -7.5542e-03],\n",
            "          [ 1.1439e-02,  5.2232e-03,  3.6333e-03]],\n",
            "\n",
            "         [[-9.8133e-03, -1.6152e-02, -1.8538e-02],\n",
            "          [ 1.2788e-03, -4.5124e-03, -7.7205e-03],\n",
            "          [ 1.1663e-02,  5.7069e-03,  3.4394e-03]],\n",
            "\n",
            "         [[-9.2452e-03, -1.6095e-02, -1.8308e-02],\n",
            "          [ 1.2292e-03, -4.3874e-03, -7.6498e-03],\n",
            "          [ 1.1206e-02,  5.1686e-03,  2.7934e-03]]],\n",
            "\n",
            "\n",
            "        [[[-1.1259e+00, -6.9135e-01, -4.3324e-01],\n",
            "          [-9.7742e-01, -4.4373e-01, -9.9720e-02],\n",
            "          [-1.2515e+00, -6.5724e-01, -2.4548e-01]],\n",
            "\n",
            "         [[-1.1262e+00, -6.9202e-01, -4.3273e-01],\n",
            "          [-9.7716e-01, -4.4326e-01, -9.9211e-02],\n",
            "          [-1.2517e+00, -6.5760e-01, -2.4616e-01]],\n",
            "\n",
            "         [[-1.1254e+00, -6.9184e-01, -4.3259e-01],\n",
            "          [-9.7739e-01, -4.4350e-01, -9.9112e-02],\n",
            "          [-1.2513e+00, -6.5697e-01, -2.4654e-01]]],\n",
            "\n",
            "\n",
            "        [[[ 3.9086e+00,  3.6631e+00,  3.8276e+00],\n",
            "          [-2.8951e-01, -4.6042e-01, -3.7931e-01],\n",
            "          [-8.0300e-01, -8.6755e-01, -7.3336e-01]],\n",
            "\n",
            "         [[ 3.9093e+00,  3.6634e+00,  3.8277e+00],\n",
            "          [-2.8933e-01, -4.6071e-01, -3.7915e-01],\n",
            "          [-8.0216e-01, -8.6728e-01, -7.3375e-01]],\n",
            "\n",
            "         [[ 3.9086e+00,  3.6636e+00,  3.8271e+00],\n",
            "          [-2.8888e-01, -4.6128e-01, -3.7922e-01],\n",
            "          [-8.0225e-01, -8.6697e-01, -7.3371e-01]]],\n",
            "\n",
            "\n",
            "        [[[ 3.4944e+01,  3.6979e+01,  3.7656e+01],\n",
            "          [ 1.2296e+01,  1.7071e+01,  1.9964e+01],\n",
            "          [-2.9976e+01, -2.3894e+01, -1.8624e+01]],\n",
            "\n",
            "         [[ 3.4943e+01,  3.6979e+01,  3.7657e+01],\n",
            "          [ 1.2296e+01,  1.7071e+01,  1.9965e+01],\n",
            "          [-2.9976e+01, -2.3894e+01, -1.8624e+01]],\n",
            "\n",
            "         [[ 3.4943e+01,  3.6978e+01,  3.7656e+01],\n",
            "          [ 1.2296e+01,  1.7071e+01,  1.9965e+01],\n",
            "          [-2.9976e+01, -2.3895e+01, -1.8624e+01]]],\n",
            "\n",
            "\n",
            "        [[[        nan,         nan,         nan],\n",
            "          [        nan,         nan,         nan],\n",
            "          [        nan,         nan,         nan]],\n",
            "\n",
            "         [[        nan,         nan,         nan],\n",
            "          [        nan,         nan,         nan],\n",
            "          [        nan,         nan,         nan]],\n",
            "\n",
            "         [[        nan,         nan,         nan],\n",
            "          [        nan,         nan,         nan],\n",
            "          [        nan,         nan,         nan]]],\n",
            "\n",
            "\n",
            "        [[[-7.4318e-02, -7.0179e-02, -8.3696e-02],\n",
            "          [-6.0671e-02, -5.5885e-02, -6.9276e-02],\n",
            "          [-5.6887e-02, -5.0390e-02, -6.1881e-02]],\n",
            "\n",
            "         [[-7.4437e-02, -7.0861e-02, -8.3790e-02],\n",
            "          [-6.0313e-02, -5.6026e-02, -6.8711e-02],\n",
            "          [-5.6542e-02, -5.0262e-02, -6.1847e-02]],\n",
            "\n",
            "         [[-7.3782e-02, -7.0579e-02, -8.3483e-02],\n",
            "          [-6.1064e-02, -5.6664e-02, -6.8606e-02],\n",
            "          [-5.6806e-02, -5.0298e-02, -6.2018e-02]]],\n",
            "\n",
            "\n",
            "        [[[-1.6570e-02, -1.0164e-02, -4.0051e-03],\n",
            "          [-3.4525e-02, -1.9502e-02, -7.2815e-03],\n",
            "          [-6.7883e-02, -4.5220e-02, -2.5372e-02]],\n",
            "\n",
            "         [[-1.6280e-02, -1.0116e-02, -3.9163e-03],\n",
            "          [-3.4389e-02, -2.0391e-02, -7.8726e-03],\n",
            "          [-6.7247e-02, -4.5903e-02, -2.5282e-02]],\n",
            "\n",
            "         [[-1.6609e-02, -9.5195e-03, -4.1064e-03],\n",
            "          [-3.3884e-02, -2.0137e-02, -7.5361e-03],\n",
            "          [-6.7768e-02, -4.5798e-02, -2.5538e-02]]],\n",
            "\n",
            "\n",
            "        [[[-7.3930e-01, -5.7388e-01, -4.8708e-01],\n",
            "          [-8.6012e-01, -6.8173e-01, -5.5928e-01],\n",
            "          [-1.0047e+00, -8.3657e-01, -6.9471e-01]],\n",
            "\n",
            "         [[-7.3966e-01, -5.7463e-01, -4.8695e-01],\n",
            "          [-8.6039e-01, -6.8252e-01, -5.5918e-01],\n",
            "          [-1.0054e+00, -8.3651e-01, -6.9428e-01]],\n",
            "\n",
            "         [[-7.3959e-01, -5.7419e-01, -4.8753e-01],\n",
            "          [-8.5992e-01, -6.8217e-01, -5.5899e-01],\n",
            "          [-1.0052e+00, -8.3661e-01, -6.9404e-01]]],\n",
            "\n",
            "\n",
            "        [[[-1.2607e+02, -1.3512e+02, -1.2578e+02],\n",
            "          [-1.6208e+02, -1.6525e+02, -1.5032e+02],\n",
            "          [-1.1386e+02, -1.1647e+02, -9.4669e+01]],\n",
            "\n",
            "         [[-1.2607e+02, -1.3512e+02, -1.2578e+02],\n",
            "          [-1.6208e+02, -1.6525e+02, -1.5032e+02],\n",
            "          [-1.1386e+02, -1.1647e+02, -9.4668e+01]],\n",
            "\n",
            "         [[-1.2607e+02, -1.3512e+02, -1.2578e+02],\n",
            "          [-1.6208e+02, -1.6525e+02, -1.5032e+02],\n",
            "          [-1.1385e+02, -1.1647e+02, -9.4668e+01]]]], device='cuda:0')}, 1: {'momentum_buffer': tensor([        nan,         nan,  3.2225e-01,  1.8611e-01, -2.5553e+00,\n",
            "         8.5208e+00,  3.1583e-01, -3.0309e-02, -3.1028e+00,  4.0379e+00,\n",
            "         3.6862e+00,         nan, -1.8254e-01, -3.0157e-01, -1.1141e-01,\n",
            "        -4.8482e+02], device='cuda:0')}, 2: {'momentum_buffer': tensor([        nan,         nan, -8.9945e-01, -1.6818e+00, -3.7411e+01,\n",
            "        -1.9078e+01, -6.7891e-01, -8.9286e-02, -8.7746e+00, -1.2303e+01,\n",
            "        -2.0459e+01,         nan, -3.7029e-01, -5.2957e-01, -2.3146e-01,\n",
            "        -3.4103e+02], device='cuda:0')}, 3: {'momentum_buffer': tensor([[[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]]], device='cuda:0')}, 4: {'momentum_buffer': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
            "       device='cuda:0')}, 5: {'momentum_buffer': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
            "       device='cuda:0')}, 6: {'momentum_buffer': tensor([[[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]]], device='cuda:0')}, 7: {'momentum_buffer': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
            "       device='cuda:0')}, 8: {'momentum_buffer': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
            "       device='cuda:0')}, 9: {'momentum_buffer': tensor([[[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]]], device='cuda:0')}, 10: {'momentum_buffer': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
            "       device='cuda:0')}, 11: {'momentum_buffer': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
            "       device='cuda:0')}, 12: {'momentum_buffer': tensor([[[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]]], device='cuda:0')}, 13: {'momentum_buffer': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
            "       device='cuda:0')}, 14: {'momentum_buffer': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
            "       device='cuda:0')}, 15: {'momentum_buffer': tensor([[[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]]], device='cuda:0')}, 16: {'momentum_buffer': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0')}, 17: {'momentum_buffer': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0')}, 18: {'momentum_buffer': tensor([[[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]]], device='cuda:0')}, 19: {'momentum_buffer': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0')}, 20: {'momentum_buffer': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0')}, 21: {'momentum_buffer': tensor([[[[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]]],\n",
            "\n",
            "\n",
            "        [[[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]]],\n",
            "\n",
            "\n",
            "        [[[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]]],\n",
            "\n",
            "\n",
            "        [[[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]]],\n",
            "\n",
            "\n",
            "        [[[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]]],\n",
            "\n",
            "\n",
            "        [[[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]]],\n",
            "\n",
            "\n",
            "        [[[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]]],\n",
            "\n",
            "\n",
            "        [[[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]]],\n",
            "\n",
            "\n",
            "        [[[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]]],\n",
            "\n",
            "\n",
            "        [[[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]]],\n",
            "\n",
            "\n",
            "        [[[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]]],\n",
            "\n",
            "\n",
            "        [[[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]]],\n",
            "\n",
            "\n",
            "        [[[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]]],\n",
            "\n",
            "\n",
            "        [[[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]]],\n",
            "\n",
            "\n",
            "        [[[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]]],\n",
            "\n",
            "\n",
            "        [[[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]]],\n",
            "\n",
            "\n",
            "        [[[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]]],\n",
            "\n",
            "\n",
            "        [[[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]]],\n",
            "\n",
            "\n",
            "        [[[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]]],\n",
            "\n",
            "\n",
            "        [[[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]]],\n",
            "\n",
            "\n",
            "        [[[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]]],\n",
            "\n",
            "\n",
            "        [[[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]]],\n",
            "\n",
            "\n",
            "        [[[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]]],\n",
            "\n",
            "\n",
            "        [[[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]]],\n",
            "\n",
            "\n",
            "        [[[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]]],\n",
            "\n",
            "\n",
            "        [[[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]]],\n",
            "\n",
            "\n",
            "        [[[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]]],\n",
            "\n",
            "\n",
            "        [[[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]]],\n",
            "\n",
            "\n",
            "        [[[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]]],\n",
            "\n",
            "\n",
            "        [[[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]]],\n",
            "\n",
            "\n",
            "        [[[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]]],\n",
            "\n",
            "\n",
            "        [[[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]]]], device='cuda:0')}, 22: {'momentum_buffer': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0')}, 23: {'momentum_buffer': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0')}, 24: {'momentum_buffer': tensor([[[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]]], device='cuda:0')}, 25: {'momentum_buffer': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0')}, 26: {'momentum_buffer': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0')}, 27: {'momentum_buffer': tensor([[[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]]], device='cuda:0')}, 28: {'momentum_buffer': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0')}, 29: {'momentum_buffer': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0')}, 30: {'momentum_buffer': tensor([[[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]]], device='cuda:0')}, 31: {'momentum_buffer': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
            "       device='cuda:0')}, 32: {'momentum_buffer': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
            "       device='cuda:0')}, 33: {'momentum_buffer': tensor([[[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]]], device='cuda:0')}, 34: {'momentum_buffer': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
            "       device='cuda:0')}, 35: {'momentum_buffer': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
            "       device='cuda:0')}, 36: {'momentum_buffer': tensor([[[[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]]],\n",
            "\n",
            "\n",
            "        [[[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]]],\n",
            "\n",
            "\n",
            "        [[[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]]],\n",
            "\n",
            "\n",
            "        [[[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]]],\n",
            "\n",
            "\n",
            "        [[[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]]]], device='cuda:0')}, 37: {'momentum_buffer': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
            "       device='cuda:0')}, 38: {'momentum_buffer': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
            "       device='cuda:0')}, 39: {'momentum_buffer': tensor([[[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]]], device='cuda:0')}, 40: {'momentum_buffer': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
            "       device='cuda:0')}, 41: {'momentum_buffer': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
            "       device='cuda:0')}, 42: {'momentum_buffer': tensor([[[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]]], device='cuda:0')}, 43: {'momentum_buffer': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
            "       device='cuda:0')}, 44: {'momentum_buffer': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
            "       device='cuda:0')}, 45: {'momentum_buffer': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
            "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0')}, 46: {'momentum_buffer': tensor([nan, nan], device='cuda:0')}}\n",
            "param_groups \t [{'lr': 1.0000000000000004e-06, 'momentum': 0.9, 'dampening': 0, 'weight_decay': 0.005, 'nesterov': False, 'initial_lr': 0.1, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46]}]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6AgPoA0u18d"
      },
      "source": [
        "## 모델 저장"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hd32m4Gy6Ua0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "127556d3-ca27-42a4-82ed-de7c7d7565b4"
      },
      "source": [
        "# save model\n",
        "\n",
        "PATH = './model_v0.04.pth'\n",
        "torch.save(model.state_dict(), PATH)\n",
        "\"\"\"\n",
        "불러올 땐\n",
        "model = TheModelClass(*args, **kwargs)\n",
        "model.load_state_dict(torch.load(PATH))\n",
        "model.eval()\n",
        "\"\"\""
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n불러올 땐\\nmodel = TheModelClass(*args, **kwargs)\\nmodel.load_state_dict(torch.load(PATH))\\nmodel.eval()\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JdR4jNelXBNR"
      },
      "source": [
        "# construct model on cuda if available\n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "\n",
        "class CNNClassifier(nn.Module):\n",
        "    \n",
        "    def __init__(self):\n",
        "        # 항상 torch.nn.Module을 상속받고 시작\n",
        "        super(CNNClassifier, self).__init__()\n",
        "        conv1 = nn.Conv2d(3, 6, 5, 1) # in channels, out_channels, kernel_size, stride\n",
        "        # activation ReLU\n",
        "        pool1 = nn.MaxPool2d(2) # 6@12*12\n",
        "        conv2 = nn.Conv2d(6, 16, 5, 1) # in channels, out_channels, kernel_size, stride\n",
        "        # activation ReLU\n",
        "        pool2 = nn.MaxPool2d(2) # 16@4*4\n",
        "        \n",
        "        self.conv_module = nn.Sequential(\n",
        "            conv1,\n",
        "            nn.ReLU(),\n",
        "            pool1,\n",
        "            conv2,\n",
        "            nn.ReLU(),\n",
        "            pool2\n",
        "        )\n",
        "        \n",
        "        fc1 = nn.Linear(32768, 1024)\n",
        "        # activation ReLU\n",
        "        fc2 = nn.Linear(1024, 64)\n",
        "        # activation ReLU\n",
        "        fc3 = nn.Linear(64, 10)\n",
        "\n",
        "        self.fc_module = nn.Sequential(\n",
        "            fc1,\n",
        "            nn.ReLU(),\n",
        "            fc2,\n",
        "            nn.ReLU(),\n",
        "            fc3\n",
        "        )\n",
        "        \n",
        "        # gpu로 할당\n",
        "        if use_cuda:\n",
        "            self.conv_module = self.conv_module.cuda()\n",
        "            self.fc_module = self.fc_module.cuda()\n",
        "        \n",
        "    def forward(self, x):\n",
        "        out = self.conv_module(x) # @16*4*4\n",
        "        # make linear\n",
        "        dim = 1\n",
        "        for d in out.size()[1:]: #16, 4, 4\n",
        "            dim = dim * d\n",
        "        out = out.view(-1, dim)\n",
        "        out = self.fc_module(out)\n",
        "        return F.softmax(out, dim=1)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6lvBD-IjY_1y"
      },
      "source": [
        "model2 = CNNClassifier()"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZ6eAxMpZBhj"
      },
      "source": [
        "learning_rate = 1e-3\n",
        "optimizer = optim.Adam(model2.parameters(), lr=learning_rate)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VvrBAEF1ZB2O",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 774
        },
        "outputId": "428f5ff4-bcfc-4d77-8bdc-4c1e9f12bae6"
      },
      "source": [
        "# create figure for plotting\n",
        "import itertools\n",
        "row_num = 2\n",
        "col_num = 4\n",
        "fig, ax = plt.subplots(row_num, col_num, figsize=(6,6))\n",
        "for i, j in itertools.product(range(row_num), range(col_num)):\n",
        "    ax[i,j].get_xaxis().set_visible(False)\n",
        "    ax[i,j].get_yaxis().set_visible(False) \n",
        "    \n",
        "trn_loss_list = []\n",
        "val_loss_list = []\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    trn_loss = 0.0\n",
        "    for i, data in enumerate(train_loader):\n",
        "        x, label = data\n",
        "        if use_cuda:\n",
        "            x = x.cuda()\n",
        "            label = label.cuda()\n",
        "        # grad init\n",
        "        optimizer.zero_grad()\n",
        "        # forward propagation\n",
        "        model_output = model2(x)\n",
        "        # calculate loss\n",
        "        loss = criterion(model_output, label)\n",
        "        # back propagation \n",
        "        loss.backward()\n",
        "        # weight update\n",
        "        optimizer.step()\n",
        "        \n",
        "        # trn_loss summary\n",
        "        train_loss += loss.item()\n",
        "        # del (memory issue)\n",
        "        del loss\n",
        "        del model_output\n",
        "        \n",
        "        # 학습과정 출력\n",
        "        if (i+1) % 100 == 0: # every 100 mini-batches\n",
        "            with torch.no_grad(): # very very very very important!!!\n",
        "                test_loss = 0.0\n",
        "                for j, val in enumerate(test_loader):\n",
        "                    test_x, test_label = test\n",
        "                    if use_cuda:\n",
        "                        test_x = test_x.cuda()\n",
        "                        test_label = test_label.cuda()\n",
        "                    test_output = cnn(val_x)\n",
        "                    t_loss = criterion(test_output, test_label)\n",
        "                    test_loss += t_loss\n",
        "\n",
        "            # draw last val dataset\n",
        "            for k in range(row_num*col_num):\n",
        "                ii = k//col_num\n",
        "                jj = k%col_num\n",
        "                ax[ii,jj].cla() # clear the current axis\n",
        "                ax[ii,jj].imshow(val_x[k,:].data.cpu().numpy().reshape(28,28), cmap='Greys')\n",
        "            \n",
        "            display.clear_output(wait=True)\n",
        "            display.display(plt.gcf()) # get a reference to a current figure\n",
        "                \n",
        "            print(\"label: {}\".format(val_label[:row_num*col_num]))\n",
        "            print(\"prediction: {}\".format(val_output.argmax(dim=1)[:row_num*col_num]))\n",
        "            del val_output\n",
        "            del v_loss\n",
        "            \n",
        "            print(\"epoch: {}/{} | step: {}/{} | trn loss: {:.4f} | val loss: {:.4f}\".format(\n",
        "                epoch+1, num_epochs, i+1, num_batches, trn_loss / 100, val_loss / len(val_loader)\n",
        "            ))            \n",
        "            \n",
        "            train_loss_list.append(train_loss/100)\n",
        "            test_loss_list.append(test_loss/len(test_loader))\n",
        "            train_loss = 0.0"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-ae2a27b7eb27>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;31m# back propagation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;31m# weight update\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    147\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Function AddmmBackward returned an invalid gradient at index 1 - got [8, 32768] but expected shape compatible with [8, 250000]"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x432 with 8 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJ0EptHBZB-j"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nXOMWNpOZCFO"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s5kS454XZCK7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g0g7DICNZCQb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZkJ35KctZCVz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "14OyExMFXJqU"
      },
      "source": [
        "model2 = ResNet2().to(DEVICE)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1,\n",
        "                      momentum=0.9, weight_decay=0.005)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}