{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN Classification v0.04.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPMnTb+wVtJ5IW+TuqA+Z0k",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chanwoo522/Hackathon/blob/main/CNN_Classification_v0_07.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_7ExL9mBFv8"
      },
      "source": [
        "## 라이브러리 임포트"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3EJ9vJD8RaD"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torchvision import transforms, datasets, utils\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "import re\n",
        "import shutil"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OPIaATPUIzYU"
      },
      "source": [
        "USE_CUDA = torch.cuda.is_available()\n",
        "DEVICE = torch.device(\"cuda\" if USE_CUDA else \"cpu\")"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YjFu_3-tBSZ3"
      },
      "source": [
        "## 구글 드라이브 마운트"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pK5FN0GbN1qj",
        "outputId": "e93a5aef-87ac-4252-a43a-7548a5f30f44"
      },
      "source": [
        "# load image files\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "boFZ0eN1BQeI"
      },
      "source": [
        "## 작업 폴더 설정"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ocIf9WqpDssw"
      },
      "source": [
        "## directory 설정\n",
        "cur_dir = os.path.abspath('/content/drive/Shareddrives/aircraft')\n",
        "image_dir = os.path.join(cur_dir, 'edge')\n",
        "image_files = [fname for fname in os.listdir(image_dir) if os.path.splitext(fname)[-1] == '.png']"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OwbgFf6Z8Xoi",
        "outputId": "d5c576c9-b3fa-4dc2-a417-2f405bc4fd4a"
      },
      "source": [
        "# labeling\n",
        "\n",
        "Labels = set()\n",
        "\n",
        "for image_file in image_files:\n",
        "    file_name = os.path.splitext(image_file)[0]\n",
        "    class_name = re.sub('_\\d+', '', file_name)\n",
        "    Labels.add(class_name)\n",
        "Labels = list(Labels)\n",
        "\n",
        "# ['j10','j11','j15','j16','j20','j31','JL10','j6',\n",
        "#  'y8g','y9jb','y20','kj2000','bjk005','ch3','wingloong',\n",
        "#  'xianglong','z9','z18','mig31','su24','su27','su30',\n",
        "#  'su35','su57','tu95ms','tu142','a50','il38','il20',\n",
        "#  'f2','e767','ec1','ch47j','p1','f4','f5','fa50','f15',\n",
        "#  'f16','fa18','f22','f35','a10','b1','b2','c130','p3',\n",
        "#  'p8','rc135','e737','kc330','u2v']\n",
        "print(Labels)\n",
        "img_size = 512"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['f-35 lightning', 'b-1 lancer']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EbGrBPR3bnEr"
      },
      "source": [
        "## Experiment directory setting\n",
        "\n",
        "train_dir = os.path.join(cur_dir, 'train_dir')\n",
        "test_dir = os.path.join(cur_dir, 'test_dir')\n",
        "\n",
        "os.makedirs(train_dir, exist_ok=True)\n",
        "os.makedirs(test_dir, exist_ok=True)\n",
        "\n",
        "for label in Labels:\n",
        "  label_dir = os.path.join(train_dir, label)\n",
        "  os.makedirs(label_dir, exist_ok=True)\n",
        "  label_dir = os.path.join(test_dir, label)\n",
        "  os.makedirs(label_dir, exist_ok=True)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RdjznjFJCIHo"
      },
      "source": [
        "## 이미지 파일 train, test data로 분리"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-axG8zDBp52"
      },
      "source": [
        "# practice file separate\n",
        "cnt = 0\n",
        "previous_class = \"\"\n",
        "image_files.sort()\n",
        "\n",
        "for image_file in image_files:\n",
        "  file_name = os.path.splitext(image_file)[0]\n",
        "  class_name = re.sub('_\\d+', '', file_name)\n",
        "  if class_name == previous_class:\n",
        "    cnt += 1\n",
        "  else:\n",
        "    cnt = 1\n",
        "  if cnt <= 200:\n",
        "    for label in Labels:\n",
        "        if label == class_name:\n",
        "          cpath = os.path.join(train_dir, label)\n",
        "          image_path = os.path.join(image_dir, image_file)\n",
        "          shutil.copy(image_path, cpath)\n",
        "        else:\n",
        "          pass\n",
        "  else:\n",
        "    for label in Labels:\n",
        "        if label == class_name:\n",
        "          cpath = os.path.join(test_dir, label)\n",
        "          image_path = os.path.join(image_dir, image_file)\n",
        "          shutil.copy(image_path, cpath)\n",
        "        else:\n",
        "          pass\n",
        "  previous_class = class_name"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZ0tmzvMCYuG"
      },
      "source": [
        "# Data load and transform\n",
        "transform0 = transforms.Compose([\n",
        "                                transforms.Resize((512,512)), \n",
        "                                transforms.RandomHorizontalFlip(),\n",
        "                                transforms.ToTensor(),\n",
        "                                transforms.Normalize((0.5,0.5,0.5),\n",
        "                                                     (0.5,0.5,0.5)),\n",
        "                                transforms.Grayscale(num_output_channels=1)\n",
        "])\n",
        "\n",
        "transform1 = transforms.Compose([\n",
        "                                transforms.RandomCrop(224),\n",
        "                                transforms.RandomHorizontalFlip(),\n",
        "                                transforms.ToTensor(),\n",
        "                                transforms.Normalize((0.5,0.5,0.5),\n",
        "                                                     (0.5,0.5,0.5)),\n",
        "                                transforms.Grayscale(num_output_channels=1)\n",
        "])\n",
        "transform2 = transforms.Compose([\n",
        "                                transforms.RandomCrop(32, padding=4),\n",
        "                                transforms.RandomHorizontalFlip(),\n",
        "                                transforms.ToTensor(),\n",
        "                                transforms.Normalize((0.5,0.5,0.5),\n",
        "                                                     (0.5,0.5,0.5)),\n",
        "                                transforms.Grayscale(num_output_channels=1)\n",
        "])\n",
        "train_dataset = datasets.ImageFolder(root=train_dir, transform=transform0)\n",
        "test_dataset = datasets.ImageFolder(root=test_dir, transform=transform0)\n",
        "\n",
        "# Hyperparameter\n",
        "\n",
        "\"\"\"\n",
        "추가로 실험해봐야 할 부분\n",
        "epoch / batch size / lr / stepsize 조정\n",
        "\"\"\"\n",
        "\n",
        "EPOCHS = 50       # 40, 150, 300\n",
        "BATCH_SIZE = 8   # 16, 64, 128\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    dataset = train_dataset,\n",
        "    batch_size = BATCH_SIZE\n",
        ")\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    dataset = test_dataset,\n",
        "    batch_size = BATCH_SIZE\n",
        ")"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "me1VskId5F_r",
        "outputId": "c1bbc48f-056f-40a3-fb3b-bf3d11965b3d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(len(train_loader))\n",
        "print(len(test_loader))"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "50\n",
            "50\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "Uo0OEsxpuAnr",
        "outputId": "47cff1f4-722d-47db-f649-3f1dbabf76f7"
      },
      "source": [
        "dataiter = iter(train_loader)\n",
        "images, labels = next(dataiter)\n",
        "img = utils.make_grid(images, padding=0)\n",
        "npimg = img.numpy()\n",
        "plt.figure(figsize = (10,7))\n",
        "plt.imshow(np.transpose(npimg,(1,2,0)))\n",
        "plt.show()"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x504 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8VEjbj5_Uk5y",
        "outputId": "a37243d3-578d-4962-b1f8-c5a61d71082b"
      },
      "source": [
        "img.shape"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 512, 4096])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5vE9HvQZm_8r",
        "outputId": "62cac489-dbcf-450d-b004-dbd03f465b63"
      },
      "source": [
        "print(len(Labels))\n",
        "\"\"\"\n",
        "다른 데이터셋으로 실험 시 반드시 num_classes를 label의 수로 변경\n",
        "이미지 사이즈 조정 시 출력계층 고치기 self.linear = nn.Linear(64*49, num_classes)\n",
        "\"\"\"\n",
        "Labels"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['f-35 lightning', 'b-1 lancer']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CTlmqBMN7ksj"
      },
      "source": [
        "class BasicBlock(nn.Module):\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3,\n",
        "                               stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, planes,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, num_classes=2):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_planes = 16\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(16)\n",
        "        self.layer1 = self._make_layer(16, 2, stride=1)\n",
        "        self.layer2 = self._make_layer(32, 2, stride=2)\n",
        "        self.layer3 = self._make_layer(64, 2, stride=2)\n",
        "        self.linear = nn.Linear(64*256, num_classes)\n",
        "\n",
        "    def _make_layer(self, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(BasicBlock(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = F.avg_pool2d(out, 8)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C0SUiS5yyJI_"
      },
      "source": [
        "model = ResNet().to(DEVICE)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1,\n",
        "                      momentum=0.9, weight_decay=0.005)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SrODV4UeJ6uZ",
        "outputId": "216f62a8-a6b6-4106-dca4-81f4734656d9"
      },
      "source": [
        "print(model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ResNet(\n",
            "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "  (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (layer1): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential(\n",
            "        (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential(\n",
            "        (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "  )\n",
            "  (linear): Linear(in_features=16384, out_features=2, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FU17_tx8Nfqc"
      },
      "source": [
        "def train(model, train_loader, optimizer, epoch):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(DEVICE), target.to(DEVICE)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.cross_entropy(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Exbxq_vAYqoc"
      },
      "source": [
        "def evaluate(model, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(DEVICE), target.to(DEVICE)\n",
        "            output = model(data)\n",
        "\n",
        "            # 배치 오차를 합산\n",
        "            test_loss += F.cross_entropy(output, target,\n",
        "                                         reduction='mean').item()\n",
        "\n",
        "            # 가장 높은 값을 가진 인덱스가 바로 예측값\n",
        "            pred = output.max(1, keepdim=True)[1]\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    test_accuracy = 100. * correct / len(test_loader.dataset)\n",
        "    return test_loss, test_accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQ_PTUoYUHjm"
      },
      "source": [
        "import gc\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-UT6KQyBE5zm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d8c44aa-6df4-418c-f9d2-ceebaaa63ebf"
      },
      "source": [
        "for epoch in range(1, EPOCHS + 1):\n",
        "\n",
        "    train(model, train_loader, optimizer, epoch)\n",
        "    test_loss, test_accuracy = evaluate(model, test_loader)\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "    print('[{}] Test Loss: {:.4f}, Accuracy: {:.2f}%'.format(\n",
        "          epoch, test_loss, test_accuracy))\n",
        "    print(pred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1] Test Loss: 13765153.0900, Accuracy: 13.00%\n",
            "[2] Test Loss: 445765.4475, Accuracy: 87.00%\n",
            "[3] Test Loss: 10377663233392.6406, Accuracy: 13.00%\n",
            "[4] Test Loss: 11882912885.7600, Accuracy: 13.00%\n",
            "[5] Test Loss: 490147302473014976.0000, Accuracy: 13.00%\n",
            "[6] Test Loss: nan, Accuracy: 87.00%\n",
            "[7] Test Loss: nan, Accuracy: 87.00%\n",
            "[8] Test Loss: nan, Accuracy: 87.00%\n",
            "[9] Test Loss: nan, Accuracy: 87.00%\n",
            "[10] Test Loss: nan, Accuracy: 87.00%\n",
            "[11] Test Loss: nan, Accuracy: 87.00%\n",
            "[12] Test Loss: nan, Accuracy: 87.00%\n",
            "[13] Test Loss: nan, Accuracy: 87.00%\n",
            "[14] Test Loss: nan, Accuracy: 87.00%\n",
            "[15] Test Loss: nan, Accuracy: 87.00%\n",
            "[16] Test Loss: nan, Accuracy: 87.00%\n",
            "[17] Test Loss: nan, Accuracy: 87.00%\n",
            "[18] Test Loss: nan, Accuracy: 87.00%\n",
            "[19] Test Loss: nan, Accuracy: 87.00%\n",
            "[20] Test Loss: nan, Accuracy: 87.00%\n",
            "[21] Test Loss: nan, Accuracy: 87.00%\n",
            "[22] Test Loss: nan, Accuracy: 87.00%\n",
            "[23] Test Loss: nan, Accuracy: 87.00%\n",
            "[24] Test Loss: nan, Accuracy: 87.00%\n",
            "[25] Test Loss: nan, Accuracy: 87.00%\n",
            "[26] Test Loss: nan, Accuracy: 87.00%\n",
            "[27] Test Loss: nan, Accuracy: 87.00%\n",
            "[28] Test Loss: nan, Accuracy: 87.00%\n",
            "[29] Test Loss: nan, Accuracy: 87.00%\n",
            "[30] Test Loss: nan, Accuracy: 87.00%\n",
            "[31] Test Loss: nan, Accuracy: 87.00%\n",
            "[32] Test Loss: nan, Accuracy: 87.00%\n",
            "[33] Test Loss: nan, Accuracy: 87.00%\n",
            "[34] Test Loss: nan, Accuracy: 87.00%\n",
            "[35] Test Loss: nan, Accuracy: 87.00%\n",
            "[36] Test Loss: nan, Accuracy: 87.00%\n",
            "[37] Test Loss: nan, Accuracy: 87.00%\n",
            "[38] Test Loss: nan, Accuracy: 87.00%\n",
            "[39] Test Loss: nan, Accuracy: 87.00%\n",
            "[40] Test Loss: nan, Accuracy: 87.00%\n",
            "[41] Test Loss: nan, Accuracy: 87.00%\n",
            "[42] Test Loss: nan, Accuracy: 87.00%\n",
            "[43] Test Loss: nan, Accuracy: 87.00%\n",
            "[44] Test Loss: nan, Accuracy: 87.00%\n",
            "[45] Test Loss: nan, Accuracy: 87.00%\n",
            "[46] Test Loss: nan, Accuracy: 87.00%\n",
            "[47] Test Loss: nan, Accuracy: 87.00%\n",
            "[48] Test Loss: nan, Accuracy: 87.00%\n",
            "[49] Test Loss: nan, Accuracy: 87.00%\n",
            "[50] Test Loss: nan, Accuracy: 87.00%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6IWBtfOqkNh"
      },
      "source": [
        "## 신경망 검사"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GosNa7uWp3Qf"
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "\n",
        "dataiter = iter(test_loader)\n",
        "images, labels = dataiter.next()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5RUa3GwuiXO"
      },
      "source": [
        "## 가중치, 파라미터 확인"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DTTiSc4ezc9r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a0be814-fd5a-4702-e0b8-e4024b8b9556"
      },
      "source": [
        "print(\"Model's state dict:\")\n",
        "for param_tensor in model.state_dict():\n",
        "  print(param_tensor, '\\t', model.state_dict()[param_tensor].size())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model's state dict:\n",
            "conv1.weight \t torch.Size([16, 3, 3, 3])\n",
            "bn1.weight \t torch.Size([16])\n",
            "bn1.bias \t torch.Size([16])\n",
            "bn1.running_mean \t torch.Size([16])\n",
            "bn1.running_var \t torch.Size([16])\n",
            "bn1.num_batches_tracked \t torch.Size([])\n",
            "layer1.0.conv1.weight \t torch.Size([16, 16, 3, 3])\n",
            "layer1.0.bn1.weight \t torch.Size([16])\n",
            "layer1.0.bn1.bias \t torch.Size([16])\n",
            "layer1.0.bn1.running_mean \t torch.Size([16])\n",
            "layer1.0.bn1.running_var \t torch.Size([16])\n",
            "layer1.0.bn1.num_batches_tracked \t torch.Size([])\n",
            "layer1.0.conv2.weight \t torch.Size([16, 16, 3, 3])\n",
            "layer1.0.bn2.weight \t torch.Size([16])\n",
            "layer1.0.bn2.bias \t torch.Size([16])\n",
            "layer1.0.bn2.running_mean \t torch.Size([16])\n",
            "layer1.0.bn2.running_var \t torch.Size([16])\n",
            "layer1.0.bn2.num_batches_tracked \t torch.Size([])\n",
            "layer1.1.conv1.weight \t torch.Size([16, 16, 3, 3])\n",
            "layer1.1.bn1.weight \t torch.Size([16])\n",
            "layer1.1.bn1.bias \t torch.Size([16])\n",
            "layer1.1.bn1.running_mean \t torch.Size([16])\n",
            "layer1.1.bn1.running_var \t torch.Size([16])\n",
            "layer1.1.bn1.num_batches_tracked \t torch.Size([])\n",
            "layer1.1.conv2.weight \t torch.Size([16, 16, 3, 3])\n",
            "layer1.1.bn2.weight \t torch.Size([16])\n",
            "layer1.1.bn2.bias \t torch.Size([16])\n",
            "layer1.1.bn2.running_mean \t torch.Size([16])\n",
            "layer1.1.bn2.running_var \t torch.Size([16])\n",
            "layer1.1.bn2.num_batches_tracked \t torch.Size([])\n",
            "layer2.0.conv1.weight \t torch.Size([32, 16, 3, 3])\n",
            "layer2.0.bn1.weight \t torch.Size([32])\n",
            "layer2.0.bn1.bias \t torch.Size([32])\n",
            "layer2.0.bn1.running_mean \t torch.Size([32])\n",
            "layer2.0.bn1.running_var \t torch.Size([32])\n",
            "layer2.0.bn1.num_batches_tracked \t torch.Size([])\n",
            "layer2.0.conv2.weight \t torch.Size([32, 32, 3, 3])\n",
            "layer2.0.bn2.weight \t torch.Size([32])\n",
            "layer2.0.bn2.bias \t torch.Size([32])\n",
            "layer2.0.bn2.running_mean \t torch.Size([32])\n",
            "layer2.0.bn2.running_var \t torch.Size([32])\n",
            "layer2.0.bn2.num_batches_tracked \t torch.Size([])\n",
            "layer2.0.shortcut.0.weight \t torch.Size([32, 16, 1, 1])\n",
            "layer2.0.shortcut.1.weight \t torch.Size([32])\n",
            "layer2.0.shortcut.1.bias \t torch.Size([32])\n",
            "layer2.0.shortcut.1.running_mean \t torch.Size([32])\n",
            "layer2.0.shortcut.1.running_var \t torch.Size([32])\n",
            "layer2.0.shortcut.1.num_batches_tracked \t torch.Size([])\n",
            "layer2.1.conv1.weight \t torch.Size([32, 32, 3, 3])\n",
            "layer2.1.bn1.weight \t torch.Size([32])\n",
            "layer2.1.bn1.bias \t torch.Size([32])\n",
            "layer2.1.bn1.running_mean \t torch.Size([32])\n",
            "layer2.1.bn1.running_var \t torch.Size([32])\n",
            "layer2.1.bn1.num_batches_tracked \t torch.Size([])\n",
            "layer2.1.conv2.weight \t torch.Size([32, 32, 3, 3])\n",
            "layer2.1.bn2.weight \t torch.Size([32])\n",
            "layer2.1.bn2.bias \t torch.Size([32])\n",
            "layer2.1.bn2.running_mean \t torch.Size([32])\n",
            "layer2.1.bn2.running_var \t torch.Size([32])\n",
            "layer2.1.bn2.num_batches_tracked \t torch.Size([])\n",
            "layer3.0.conv1.weight \t torch.Size([64, 32, 3, 3])\n",
            "layer3.0.bn1.weight \t torch.Size([64])\n",
            "layer3.0.bn1.bias \t torch.Size([64])\n",
            "layer3.0.bn1.running_mean \t torch.Size([64])\n",
            "layer3.0.bn1.running_var \t torch.Size([64])\n",
            "layer3.0.bn1.num_batches_tracked \t torch.Size([])\n",
            "layer3.0.conv2.weight \t torch.Size([64, 64, 3, 3])\n",
            "layer3.0.bn2.weight \t torch.Size([64])\n",
            "layer3.0.bn2.bias \t torch.Size([64])\n",
            "layer3.0.bn2.running_mean \t torch.Size([64])\n",
            "layer3.0.bn2.running_var \t torch.Size([64])\n",
            "layer3.0.bn2.num_batches_tracked \t torch.Size([])\n",
            "layer3.0.shortcut.0.weight \t torch.Size([64, 32, 1, 1])\n",
            "layer3.0.shortcut.1.weight \t torch.Size([64])\n",
            "layer3.0.shortcut.1.bias \t torch.Size([64])\n",
            "layer3.0.shortcut.1.running_mean \t torch.Size([64])\n",
            "layer3.0.shortcut.1.running_var \t torch.Size([64])\n",
            "layer3.0.shortcut.1.num_batches_tracked \t torch.Size([])\n",
            "layer3.1.conv1.weight \t torch.Size([64, 64, 3, 3])\n",
            "layer3.1.bn1.weight \t torch.Size([64])\n",
            "layer3.1.bn1.bias \t torch.Size([64])\n",
            "layer3.1.bn1.running_mean \t torch.Size([64])\n",
            "layer3.1.bn1.running_var \t torch.Size([64])\n",
            "layer3.1.bn1.num_batches_tracked \t torch.Size([])\n",
            "layer3.1.conv2.weight \t torch.Size([64, 64, 3, 3])\n",
            "layer3.1.bn2.weight \t torch.Size([64])\n",
            "layer3.1.bn2.bias \t torch.Size([64])\n",
            "layer3.1.bn2.running_mean \t torch.Size([64])\n",
            "layer3.1.bn2.running_var \t torch.Size([64])\n",
            "layer3.1.bn2.num_batches_tracked \t torch.Size([])\n",
            "linear.weight \t torch.Size([2, 16384])\n",
            "linear.bias \t torch.Size([2])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uootomSzl6FN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f6baa18-32c8-41fa-ea36-c5aa77767ef5"
      },
      "source": [
        "print(\"Optimizer's state_dict:\")\n",
        "for var_name in optimizer.state_dict():\n",
        "  print(var_name, '\\t', optimizer.state_dict()[var_name])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Optimizer's state_dict:\n",
            "state \t {0: {'momentum_buffer': tensor([[[[-2.0367e+02, -1.9317e+02, -2.0058e+02],\n",
            "          [-2.1546e+02, -2.1226e+02, -2.2673e+02],\n",
            "          [-2.4980e+02, -2.5555e+02, -2.7632e+02]],\n",
            "\n",
            "         [[-2.0367e+02, -1.9317e+02, -2.0058e+02],\n",
            "          [-2.1546e+02, -2.1226e+02, -2.2673e+02],\n",
            "          [-2.4980e+02, -2.5555e+02, -2.7632e+02]],\n",
            "\n",
            "         [[-2.0367e+02, -1.9317e+02, -2.0058e+02],\n",
            "          [-2.1546e+02, -2.1226e+02, -2.2673e+02],\n",
            "          [-2.4980e+02, -2.5555e+02, -2.7632e+02]]],\n",
            "\n",
            "\n",
            "        [[[-1.0575e-02, -2.6706e-02, -2.5344e-02],\n",
            "          [ 2.8083e-02,  1.1068e-02,  9.9231e-03],\n",
            "          [ 3.0961e-02,  1.3636e-02,  1.2159e-02]],\n",
            "\n",
            "         [[-1.0447e-02, -2.7135e-02, -2.5825e-02],\n",
            "          [ 2.7791e-02,  1.0731e-02,  8.9658e-03],\n",
            "          [ 3.1241e-02,  1.4064e-02,  1.2071e-02]],\n",
            "\n",
            "         [[-1.1018e-02, -2.6248e-02, -2.5733e-02],\n",
            "          [ 2.7696e-02,  1.1208e-02,  9.0837e-03],\n",
            "          [ 3.1681e-02,  1.3804e-02,  1.1735e-02]]],\n",
            "\n",
            "\n",
            "        [[[-3.2049e-01, -3.2433e-01, -3.9381e-01],\n",
            "          [-4.8517e-01, -4.9056e-01, -5.6316e-01],\n",
            "          [-3.6628e-01, -3.8401e-01, -4.5356e-01]],\n",
            "\n",
            "         [[-3.2132e-01, -3.2496e-01, -3.9299e-01],\n",
            "          [-4.8554e-01, -4.9048e-01, -5.6321e-01],\n",
            "          [-3.6697e-01, -3.8479e-01, -4.5394e-01]],\n",
            "\n",
            "         [[-3.2104e-01, -3.2441e-01, -3.9341e-01],\n",
            "          [-4.8480e-01, -4.9086e-01, -5.6348e-01],\n",
            "          [-3.6658e-01, -3.8461e-01, -4.5347e-01]]],\n",
            "\n",
            "\n",
            "        [[[ 1.2826e+01,  1.3411e+01,  1.3717e+01],\n",
            "          [ 1.2030e+01,  1.2670e+01,  1.3104e+01],\n",
            "          [ 1.0702e+01,  1.1333e+01,  1.1844e+01]],\n",
            "\n",
            "         [[ 1.2827e+01,  1.3412e+01,  1.3718e+01],\n",
            "          [ 1.2030e+01,  1.2670e+01,  1.3103e+01],\n",
            "          [ 1.0703e+01,  1.1333e+01,  1.1844e+01]],\n",
            "\n",
            "         [[ 1.2827e+01,  1.3411e+01,  1.3717e+01],\n",
            "          [ 1.2030e+01,  1.2670e+01,  1.3103e+01],\n",
            "          [ 1.0703e+01,  1.1332e+01,  1.1844e+01]]],\n",
            "\n",
            "\n",
            "        [[[-1.6747e-01,  6.2674e-02,  1.5375e-01],\n",
            "          [-3.2828e-01, -1.0602e-01, -1.1215e-02],\n",
            "          [-6.2032e-01, -4.2358e-01, -3.3352e-01]],\n",
            "\n",
            "         [[-1.6734e-01,  6.2762e-02,  1.5416e-01],\n",
            "          [-3.2746e-01, -1.0565e-01, -1.0979e-02],\n",
            "          [-6.1994e-01, -4.2333e-01, -3.3419e-01]],\n",
            "\n",
            "         [[-1.6710e-01,  6.2776e-02,  1.5387e-01],\n",
            "          [-3.2757e-01, -1.0553e-01, -1.1354e-02],\n",
            "          [-6.2097e-01, -4.2288e-01, -3.3403e-01]]],\n",
            "\n",
            "\n",
            "        [[[        nan,         nan,         nan],\n",
            "          [        nan,         nan,         nan],\n",
            "          [        nan,         nan,         nan]],\n",
            "\n",
            "         [[        nan,         nan,         nan],\n",
            "          [        nan,         nan,         nan],\n",
            "          [        nan,         nan,         nan]],\n",
            "\n",
            "         [[        nan,         nan,         nan],\n",
            "          [        nan,         nan,         nan],\n",
            "          [        nan,         nan,         nan]]],\n",
            "\n",
            "\n",
            "        [[[ 9.6179e+01,  9.5755e+01,  9.2963e+01],\n",
            "          [ 9.2349e+01,  9.0132e+01,  8.6044e+01],\n",
            "          [ 8.4743e+01,  8.1029e+01,  7.6243e+01]],\n",
            "\n",
            "         [[ 9.6180e+01,  9.5755e+01,  9.2963e+01],\n",
            "          [ 9.2349e+01,  9.0132e+01,  8.6044e+01],\n",
            "          [ 8.4743e+01,  8.1028e+01,  7.6244e+01]],\n",
            "\n",
            "         [[ 9.6179e+01,  9.5755e+01,  9.2964e+01],\n",
            "          [ 9.2350e+01,  9.0132e+01,  8.6045e+01],\n",
            "          [ 8.4744e+01,  8.1029e+01,  7.6244e+01]]],\n",
            "\n",
            "\n",
            "        [[[-3.3456e-02,  3.4027e-01,  4.6954e-01],\n",
            "          [-3.9781e-02,  3.2552e-01,  4.5902e-01],\n",
            "          [-3.3040e-01,  1.8412e-03,  1.2491e-01]],\n",
            "\n",
            "         [[-3.3743e-02,  3.4050e-01,  4.6904e-01],\n",
            "          [-3.9696e-02,  3.2614e-01,  4.5842e-01],\n",
            "          [-3.3031e-01,  2.6327e-03,  1.2518e-01]],\n",
            "\n",
            "         [[-3.3723e-02,  3.3970e-01,  4.6984e-01],\n",
            "          [-4.0116e-02,  3.2635e-01,  4.5822e-01],\n",
            "          [-3.3026e-01,  2.2613e-03,  1.2430e-01]]],\n",
            "\n",
            "\n",
            "        [[[-5.1043e-02, -5.8893e-03,  1.6130e-03],\n",
            "          [-4.9949e-02, -2.5771e-03,  6.4222e-03],\n",
            "          [-7.2012e-02, -2.3575e-02, -1.3506e-02]],\n",
            "\n",
            "         [[-5.0301e-02, -6.3884e-03,  1.3062e-03],\n",
            "          [-4.9477e-02, -2.5501e-03,  5.9189e-03],\n",
            "          [-7.1491e-02, -2.3386e-02, -1.2559e-02]],\n",
            "\n",
            "         [[-5.0124e-02, -6.2389e-03,  1.9711e-03],\n",
            "          [-4.9253e-02, -3.3476e-03,  5.8422e-03],\n",
            "          [-7.1647e-02, -2.2957e-02, -1.2651e-02]]],\n",
            "\n",
            "\n",
            "        [[[-1.9226e-01, -8.6192e-02, -6.6746e-02],\n",
            "          [-1.0837e-01,  2.7146e-03,  2.8630e-02],\n",
            "          [-1.3402e-01, -3.0119e-02,  1.0756e-02]],\n",
            "\n",
            "         [[-1.9151e-01, -8.6578e-02, -6.6884e-02],\n",
            "          [-1.0789e-01,  2.1145e-03,  2.7938e-02],\n",
            "          [-1.3387e-01, -2.9890e-02,  1.0886e-02]],\n",
            "\n",
            "         [[-1.9219e-01, -8.6987e-02, -6.6254e-02],\n",
            "          [-1.0795e-01,  2.2275e-03,  2.8845e-02],\n",
            "          [-1.3332e-01, -3.0211e-02,  1.1021e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 7.2294e-04,  1.7861e-03,  3.2517e-03],\n",
            "          [-4.2748e-03, -3.3023e-03, -3.0064e-03],\n",
            "          [-1.5675e-02, -1.5094e-02, -1.5059e-02]],\n",
            "\n",
            "         [[ 3.0069e-04,  1.8905e-03,  2.9568e-03],\n",
            "          [-4.3595e-03, -3.2895e-03, -2.8841e-03],\n",
            "          [-1.6184e-02, -1.5199e-02, -1.5003e-02]],\n",
            "\n",
            "         [[ 1.0427e-03,  1.6178e-03,  3.6946e-03],\n",
            "          [-4.5749e-03, -3.2034e-03, -3.2891e-03],\n",
            "          [-1.5531e-02, -1.5207e-02, -1.5042e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 1.0609e+05,  5.0840e+05,  6.3037e+05],\n",
            "          [ 1.3950e+06,  1.6943e+06,  1.7680e+06],\n",
            "          [-3.9310e+05, -2.1517e+05, -2.5879e+05]],\n",
            "\n",
            "         [[ 1.0609e+05,  5.0840e+05,  6.3037e+05],\n",
            "          [ 1.3950e+06,  1.6943e+06,  1.7680e+06],\n",
            "          [-3.9310e+05, -2.1517e+05, -2.5879e+05]],\n",
            "\n",
            "         [[ 1.0609e+05,  5.0840e+05,  6.3037e+05],\n",
            "          [ 1.3950e+06,  1.6943e+06,  1.7680e+06],\n",
            "          [-3.9310e+05, -2.1517e+05, -2.5879e+05]]],\n",
            "\n",
            "\n",
            "        [[[        nan,         nan,         nan],\n",
            "          [        nan,         nan,         nan],\n",
            "          [        nan,         nan,         nan]],\n",
            "\n",
            "         [[        nan,         nan,         nan],\n",
            "          [        nan,         nan,         nan],\n",
            "          [        nan,         nan,         nan]],\n",
            "\n",
            "         [[        nan,         nan,         nan],\n",
            "          [        nan,         nan,         nan],\n",
            "          [        nan,         nan,         nan]]],\n",
            "\n",
            "\n",
            "        [[[ 8.9948e-01,  1.0971e+00,  1.4439e+00],\n",
            "          [ 1.5405e+00,  1.7964e+00,  2.2850e+00],\n",
            "          [ 2.6669e+00,  3.0194e+00,  3.6969e+00]],\n",
            "\n",
            "         [[ 8.9888e-01,  1.0973e+00,  1.4437e+00],\n",
            "          [ 1.5399e+00,  1.7971e+00,  2.2848e+00],\n",
            "          [ 2.6661e+00,  3.0198e+00,  3.6966e+00]],\n",
            "\n",
            "         [[ 8.9931e-01,  1.0970e+00,  1.4433e+00],\n",
            "          [ 1.5404e+00,  1.7963e+00,  2.2849e+00],\n",
            "          [ 2.6661e+00,  3.0199e+00,  3.6968e+00]]],\n",
            "\n",
            "\n",
            "        [[[ 2.1154e+03,  2.0443e+03,  1.8925e+03],\n",
            "          [ 3.6184e+03,  3.2502e+03,  2.9016e+03],\n",
            "          [ 7.8273e+03,  7.3956e+03,  6.8001e+03]],\n",
            "\n",
            "         [[ 2.1154e+03,  2.0443e+03,  1.8925e+03],\n",
            "          [ 3.6184e+03,  3.2502e+03,  2.9016e+03],\n",
            "          [ 7.8273e+03,  7.3956e+03,  6.8001e+03]],\n",
            "\n",
            "         [[ 2.1155e+03,  2.0443e+03,  1.8925e+03],\n",
            "          [ 3.6184e+03,  3.2502e+03,  2.9016e+03],\n",
            "          [ 7.8273e+03,  7.3956e+03,  6.8001e+03]]],\n",
            "\n",
            "\n",
            "        [[[ 2.9258e+01,  3.1342e+01,  3.3440e+01],\n",
            "          [ 3.2838e+01,  3.6337e+01,  3.8359e+01],\n",
            "          [ 3.4938e+01,  4.0250e+01,  4.2207e+01]],\n",
            "\n",
            "         [[ 2.9258e+01,  3.1343e+01,  3.3440e+01],\n",
            "          [ 3.2839e+01,  3.6336e+01,  3.8359e+01],\n",
            "          [ 3.4938e+01,  4.0249e+01,  4.2206e+01]],\n",
            "\n",
            "         [[ 2.9258e+01,  3.1343e+01,  3.3440e+01],\n",
            "          [ 3.2838e+01,  3.6337e+01,  3.8360e+01],\n",
            "          [ 3.4938e+01,  4.0250e+01,  4.2206e+01]]]], device='cuda:0')}, 1: {'momentum_buffer': tensor([-1.0024e+02,  1.8442e-03,  4.9667e-02,  1.0306e+00, -3.9279e-02,\n",
            "                nan,  2.0563e+00,  1.4138e-01, -1.7039e-01, -1.7939e-01,\n",
            "        -2.8500e-01,  9.4028e+05,         nan, -1.0694e-01, -4.5295e+01,\n",
            "         1.4582e+02], device='cuda:0')}, 2: {'momentum_buffer': tensor([-6.3704e+01, -4.5403e-02, -4.8873e-01, -8.5396e-01, -2.1898e-01,\n",
            "                nan, -5.5802e+00, -6.0029e-01, -2.1988e-01, -4.2943e-01,\n",
            "        -5.7830e-01, -1.9657e+06,         nan, -1.8529e+00, -2.6276e+03,\n",
            "        -1.0546e+02], device='cuda:0')}, 3: {'momentum_buffer': tensor([[[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]]], device='cuda:0')}, 4: {'momentum_buffer': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
            "       device='cuda:0')}, 5: {'momentum_buffer': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
            "       device='cuda:0')}, 6: {'momentum_buffer': tensor([[[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]]], device='cuda:0')}, 7: {'momentum_buffer': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
            "       device='cuda:0')}, 8: {'momentum_buffer': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
            "       device='cuda:0')}, 9: {'momentum_buffer': tensor([[[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]]], device='cuda:0')}, 10: {'momentum_buffer': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
            "       device='cuda:0')}, 11: {'momentum_buffer': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
            "       device='cuda:0')}, 12: {'momentum_buffer': tensor([[[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]]], device='cuda:0')}, 13: {'momentum_buffer': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
            "       device='cuda:0')}, 14: {'momentum_buffer': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
            "       device='cuda:0')}, 15: {'momentum_buffer': tensor([[[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]]], device='cuda:0')}, 16: {'momentum_buffer': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0')}, 17: {'momentum_buffer': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0')}, 18: {'momentum_buffer': tensor([[[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]]], device='cuda:0')}, 19: {'momentum_buffer': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0')}, 20: {'momentum_buffer': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0')}, 21: {'momentum_buffer': tensor([[[[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]]],\n",
            "\n",
            "\n",
            "        [[[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]]],\n",
            "\n",
            "\n",
            "        [[[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]]],\n",
            "\n",
            "\n",
            "        [[[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]]],\n",
            "\n",
            "\n",
            "        [[[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]]],\n",
            "\n",
            "\n",
            "        [[[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]]],\n",
            "\n",
            "\n",
            "        [[[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]]],\n",
            "\n",
            "\n",
            "        [[[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]]],\n",
            "\n",
            "\n",
            "        [[[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]]],\n",
            "\n",
            "\n",
            "        [[[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]]],\n",
            "\n",
            "\n",
            "        [[[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]]],\n",
            "\n",
            "\n",
            "        [[[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]]],\n",
            "\n",
            "\n",
            "        [[[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]]],\n",
            "\n",
            "\n",
            "        [[[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]]],\n",
            "\n",
            "\n",
            "        [[[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]]],\n",
            "\n",
            "\n",
            "        [[[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]]],\n",
            "\n",
            "\n",
            "        [[[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]]],\n",
            "\n",
            "\n",
            "        [[[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]]],\n",
            "\n",
            "\n",
            "        [[[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]]],\n",
            "\n",
            "\n",
            "        [[[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]]],\n",
            "\n",
            "\n",
            "        [[[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]]],\n",
            "\n",
            "\n",
            "        [[[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]]],\n",
            "\n",
            "\n",
            "        [[[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]]],\n",
            "\n",
            "\n",
            "        [[[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]]],\n",
            "\n",
            "\n",
            "        [[[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]]],\n",
            "\n",
            "\n",
            "        [[[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]]],\n",
            "\n",
            "\n",
            "        [[[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]]],\n",
            "\n",
            "\n",
            "        [[[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]]],\n",
            "\n",
            "\n",
            "        [[[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]]],\n",
            "\n",
            "\n",
            "        [[[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]]],\n",
            "\n",
            "\n",
            "        [[[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]]],\n",
            "\n",
            "\n",
            "        [[[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]]]], device='cuda:0')}, 22: {'momentum_buffer': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0')}, 23: {'momentum_buffer': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0')}, 24: {'momentum_buffer': tensor([[[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]]], device='cuda:0')}, 25: {'momentum_buffer': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0')}, 26: {'momentum_buffer': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0')}, 27: {'momentum_buffer': tensor([[[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]]], device='cuda:0')}, 28: {'momentum_buffer': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0')}, 29: {'momentum_buffer': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0')}, 30: {'momentum_buffer': tensor([[[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]]], device='cuda:0')}, 31: {'momentum_buffer': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
            "       device='cuda:0')}, 32: {'momentum_buffer': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
            "       device='cuda:0')}, 33: {'momentum_buffer': tensor([[[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]]], device='cuda:0')}, 34: {'momentum_buffer': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
            "       device='cuda:0')}, 35: {'momentum_buffer': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
            "       device='cuda:0')}, 36: {'momentum_buffer': tensor([[[[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]]],\n",
            "\n",
            "\n",
            "        [[[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]]],\n",
            "\n",
            "\n",
            "        [[[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]]],\n",
            "\n",
            "\n",
            "        [[[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]]],\n",
            "\n",
            "\n",
            "        [[[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]]]], device='cuda:0')}, 37: {'momentum_buffer': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
            "       device='cuda:0')}, 38: {'momentum_buffer': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
            "       device='cuda:0')}, 39: {'momentum_buffer': tensor([[[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]]], device='cuda:0')}, 40: {'momentum_buffer': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
            "       device='cuda:0')}, 41: {'momentum_buffer': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
            "       device='cuda:0')}, 42: {'momentum_buffer': tensor([[[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]]], device='cuda:0')}, 43: {'momentum_buffer': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
            "       device='cuda:0')}, 44: {'momentum_buffer': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
            "       device='cuda:0')}, 45: {'momentum_buffer': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
            "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0')}, 46: {'momentum_buffer': tensor([nan, nan], device='cuda:0')}}\n",
            "param_groups \t [{'lr': 1.0000000000000004e-06, 'momentum': 0.9, 'dampening': 0, 'weight_decay': 0.005, 'nesterov': False, 'initial_lr': 0.1, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46]}]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6AgPoA0u18d"
      },
      "source": [
        "## 모델 저장"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hd32m4Gy6Ua0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "307e8006-a15e-4fb8-a0b3-8c00efc1b795"
      },
      "source": [
        "# save model\n",
        "\n",
        "PATH = './model_v0.04.pth'\n",
        "torch.save(model.state_dict(), PATH)\n",
        "\"\"\"\n",
        "불러올 땐\n",
        "model = TheModelClass(*args, **kwargs)\n",
        "model.load_state_dict(torch.load(PATH))\n",
        "model.eval()\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n불러올 땐\\nmodel = TheModelClass(*args, **kwargs)\\nmodel.load_state_dict(torch.load(PATH))\\nmodel.eval()\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JdR4jNelXBNR"
      },
      "source": [
        "# construct model on cuda if available\n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "\n",
        "class CNNClassifier(nn.Module):\n",
        "    \n",
        "    def __init__(self):\n",
        "        # 항상 torch.nn.Module을 상속받고 시작\n",
        "        super(CNNClassifier, self).__init__()\n",
        "        conv1 = nn.Conv2d(3, 6, 5, stride=3) # in channels, out_channels, kernel_size, stride\n",
        "        # activation ReLU\n",
        "        pool1 = nn.MaxPool2d(2) # 6@12*12\n",
        "        conv2 = nn.Conv2d(6, 6, 5, stride=3) # in channels, out_channels, kernel_size, stride\n",
        "        # activation ReLU\n",
        "        pool2 = nn.MaxPool2d(2) # 16@4*4\n",
        "        \n",
        "        self.conv_module = nn.Sequential(\n",
        "            conv1,\n",
        "            nn.ReLU(),\n",
        "            pool1,\n",
        "            conv2,\n",
        "            nn.ReLU(),\n",
        "            pool2\n",
        "        )\n",
        "        \n",
        "        fc1 = nn.Linear(2*3*13*13, 120)\n",
        "        # activation ReLU\n",
        "        fc2 = nn.Linear(120, 84)\n",
        "        # activation ReLU\n",
        "        fc3 = nn.Linear(84, 2)\n",
        "\n",
        "        self.fc_module = nn.Sequential(\n",
        "            fc1,\n",
        "            nn.ReLU(),\n",
        "            fc2,\n",
        "            nn.ReLU(),\n",
        "            fc3\n",
        "        )\n",
        "        \n",
        "        # gpu로 할당\n",
        "        if use_cuda:\n",
        "            self.conv_module = self.conv_module.cuda()\n",
        "            self.fc_module = self.fc_module.cuda()\n",
        "        \n",
        "    def forward(self, x):\n",
        "        out = self.conv_module(x) # @16*4*4\n",
        "        # make linear\n",
        "        dim = 1\n",
        "        for d in out.size()[1:]: #16, 4, 4\n",
        "            dim = dim * d\n",
        "        out = out.view(-1, dim)\n",
        "        out = self.fc_module(out)\n",
        "        return F.softmax(out, dim=1)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6lvBD-IjY_1y"
      },
      "source": [
        "model2 = CNNClassifier()"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZ6eAxMpZBhj"
      },
      "source": [
        "learning_rate = 0.01\n",
        "optimizer = optim.Adam(model2.parameters(), lr=learning_rate)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B456Y7f04AHY"
      },
      "source": [
        "import gc\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Q6JzHk38Oar",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 361
        },
        "outputId": "baf0bdac-3ad6-47dd-e2ee-276d1ea4527c"
      },
      "source": [
        "# create figure for plotting\n",
        "import itertools\n",
        "row_num = 2\n",
        "col_num = 4\n",
        "fig, ax = plt.subplots(row_num, col_num, figsize=(6,6))\n",
        "for i, j in itertools.product(range(row_num), range(col_num)):\n",
        "    ax[i,j].get_xaxis().set_visible(False)\n",
        "    ax[i,j].get_yaxis().set_visible(False) \n",
        "    \n"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x432 with 8 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mY7SrLVHFvcM"
      },
      "source": [
        "import math\n",
        "train_loss_list = []\n",
        "test_loss_list = []\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dr68DpCDIgPv"
      },
      "source": [
        "from IPython.display import Image\n",
        "from IPython import display\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VvrBAEF1ZB2O",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "outputId": "ffd9b3a9-ce90-4860-cf21-b12e54bea410"
      },
      "source": [
        "num_batches = len(train_loader)\n",
        "for epoch in range(EPOCHS):\n",
        "    train_loss = 0.0\n",
        "    for i, data in enumerate(train_loader):\n",
        "        x, label = data\n",
        "        if use_cuda:\n",
        "            x = x.cuda()\n",
        "            label = label.cuda()\n",
        "        # grad init\n",
        "        optimizer.zero_grad()\n",
        "        # forward propagation\n",
        "        model_output = model2(x)\n",
        "        # calculate loss\n",
        "        loss = criterion(model_output, label)\n",
        "        # back propagation \n",
        "        loss.backward()\n",
        "        # weight update\n",
        "        optimizer.step()\n",
        "        \n",
        "        # train_loss summary\n",
        "        train_loss += loss.item()\n",
        "        # del (memory issue)\n",
        "        del loss\n",
        "        del model_output\n",
        "        \n",
        "        # 학습과정 출력\n",
        "        if (i+1) % 10 == 0: # every 10 or 100 mini-batches\n",
        "            with torch.no_grad(): # very very very very important!!!\n",
        "                test_loss = 0.0\n",
        "                for j, test in enumerate(test_loader):\n",
        "                    test_x, test_label = test\n",
        "                    if use_cuda:\n",
        "                        test_x = test_x.cuda()\n",
        "                        test_label = test_label.cuda()\n",
        "                    test_output = model2(test_x)\n",
        "                    t_loss = criterion(test_output, test_label)\n",
        "                    test_loss += t_loss\n",
        "\n",
        "            # draw last test dataset\n",
        "            for k in range(row_num*col_num):\n",
        "                ii = k//col_num\n",
        "                jj = k%col_num\n",
        "                ax[ii,jj].cla() # clear the current axis\n",
        "                ax[ii,jj].plt.imshow(test_x[k,:].data.cpu().numpy().reshape(2**9, 3*(2**9)), cmap='Greys')\n",
        "            \n",
        "            display.clear_output(wait=True)\n",
        "            display.display(plt.gcf()) # get a reference to a current figure\n",
        "                \n",
        "            print(\"test label: {}\".format(test_label[:row_num*col_num]))\n",
        "            print(\"prediction: {}\".format(test_output.argmax(dim=1)[:row_num*col_num]))\n",
        "            del test_output\n",
        "            del t_loss\n",
        "            \n",
        "            print(\"epoch: {}/{} | step: {}/{} | train loss: {:.4f} | test loss: {:.4f}\".format(\n",
        "                epoch+1, EPOCHS, i+1, num_batches, train_loss / len(train_loader), test_loss / len(test_loader)\n",
        "            ))            \n",
        "            \n",
        "            train_loss_list.append(train_loss/100)\n",
        "            test_loss_list.append(test_loss/len(test_loader))\n",
        "            train_loss = 0.0"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "label: tensor([1, 1, 1, 1, 1, 1, 1, 1])\n",
            "prediction: tensor([0, 0, 0, 0, 0, 0, 0, 0])\n",
            "epoch: 50/50 | step: 50/50 | train loss: 0.1313 | test loss: 0.4433\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJ0EptHBZB-j",
        "outputId": "a12a3f76-b2e8-4db2-c77e-4c5b138c957b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        }
      },
      "source": [
        "print(epoch)\n",
        "print(num_batches)\n",
        "print(train_loss)\n",
        "print(test_label)\n",
        "print(test_output)\n",
        "print(len(train_loader))"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "49\n",
            "50\n",
            "0.0\n",
            "tensor([1, 1, 1, 1, 1, 1, 1, 1])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-65-3fddea4ea195>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'test_output' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s5kS454XZCK7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g0g7DICNZCQb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZkJ35KctZCVz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "14OyExMFXJqU"
      },
      "source": [
        "model2 = ResNet2().to(DEVICE)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1,\n",
        "                      momentum=0.9, weight_decay=0.005)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}