{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "CNN Classification v0.04.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOwiUq6sg9/mVmzbkbQs3Tf",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chanwoo522/Hackathon/blob/main/CNN_Classification_v0_04.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_7ExL9mBFv8"
      },
      "source": [
        "## 라이브러리 임포트"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3EJ9vJD8RaD"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torchvision import transforms, datasets, utils\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "import re\n",
        "import shutil"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OPIaATPUIzYU"
      },
      "source": [
        "USE_CUDA = torch.cuda.is_available()\n",
        "DEVICE = torch.device(\"cuda\" if USE_CUDA else \"cpu\")"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YjFu_3-tBSZ3"
      },
      "source": [
        "## 구글 드라이브 마운트"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pK5FN0GbN1qj",
        "outputId": "1005a436-7f3f-4ceb-f757-410c5c991efe"
      },
      "source": [
        "# load image files\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "boFZ0eN1BQeI"
      },
      "source": [
        "## 작업 폴더 설정"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ocIf9WqpDssw"
      },
      "source": [
        "## directory 설정\n",
        "cur_dir = os.path.abspath('/content/drive/Shareddrives/aircraft')\n",
        "image_dir = os.path.join(cur_dir, 'edge')\n",
        "image_files = [fname for fname in os.listdir(image_dir) if os.path.splitext(fname)[-1] == '.png']"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OwbgFf6Z8Xoi",
        "outputId": "256f99da-f3bd-4ecf-b717-a16dbd3e060b"
      },
      "source": [
        "# labeling\n",
        "\n",
        "Labels = set()\n",
        "\n",
        "for image_file in image_files:\n",
        "    file_name = os.path.splitext(image_file)[0]\n",
        "    class_name = re.sub('_\\d+', '', file_name)\n",
        "    Labels.add(class_name)\n",
        "Labels = list(Labels)\n",
        "\n",
        "# ['j10','j11','j15','j16','j20','j31','JL10','j6',\n",
        "#  'y8g','y9jb','y20','kj2000','bjk005','ch3','wingloong',\n",
        "#  'xianglong','z9','z18','mig31','su24','su27','su30',\n",
        "#  'su35','su57','tu95ms','tu142','a50','il38','il20',\n",
        "#  'f2','e767','ec1','ch47j','p1','f4','f5','fa50','f15',\n",
        "#  'f16','fa18','f22','f35','a10','b1','b2','c130','p3',\n",
        "#  'p8','rc135','e737','kc330','u2v']\n",
        "print(Labels)\n",
        "img_size = 512"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['f-35 lightning', 'b-1 lancer']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EbGrBPR3bnEr"
      },
      "source": [
        "## Experiment directory setting\n",
        "\n",
        "train_dir = os.path.join(cur_dir, 'train_dir')\n",
        "test_dir = os.path.join(cur_dir, 'test_dir')\n",
        "\n",
        "os.makedirs(train_dir, exist_ok=True)\n",
        "os.makedirs(test_dir, exist_ok=True)\n",
        "\n",
        "for label in Labels:\n",
        "  label_dir = os.path.join(train_dir, label)\n",
        "  os.makedirs(label_dir, exist_ok=True)\n",
        "  label_dir = os.path.join(test_dir, label)\n",
        "  os.makedirs(label_dir, exist_ok=True)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RdjznjFJCIHo"
      },
      "source": [
        "## 이미지 파일 train, test data로 분리"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-axG8zDBp52"
      },
      "source": [
        "# practice file separate\n",
        "cnt = 0\n",
        "previous_class = \"\"\n",
        "image_files.sort()\n",
        "\n",
        "for image_file in image_files:\n",
        "  file_name = os.path.splitext(image_file)[0]\n",
        "  class_name = re.sub('_\\d+', '', file_name)\n",
        "  if class_name == previous_class:\n",
        "    cnt += 1\n",
        "  else:\n",
        "    cnt = 1\n",
        "  if cnt <= 200:\n",
        "    for label in Labels:\n",
        "        if label == class_name:\n",
        "          cpath = os.path.join(train_dir, label)\n",
        "          image_path = os.path.join(image_dir, image_file)\n",
        "          shutil.copy(image_path, cpath)\n",
        "        else:\n",
        "          pass\n",
        "  else:\n",
        "    for label in Labels:\n",
        "        if label == class_name:\n",
        "          cpath = os.path.join(test_dir, label)\n",
        "          image_path = os.path.join(image_dir, image_file)\n",
        "          shutil.copy(image_path, cpath)\n",
        "        else:\n",
        "          pass\n",
        "  previous_class = class_name"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZ0tmzvMCYuG"
      },
      "source": [
        "# Data load and transform\n",
        "transform0 = transforms.Compose([\n",
        "                                transforms.Resize((512,512)), \n",
        "                                transforms.RandomHorizontalFlip(),\n",
        "                                transforms.ToTensor(),\n",
        "                                transforms.Normalize((0.5,0.5,0.5),\n",
        "                                                     (0.5,0.5,0.5))\n",
        "])\n",
        "\n",
        "transform1 = transforms.Compose([\n",
        "                                transforms.RandomCrop(224),\n",
        "                                transforms.RandomHorizontalFlip(),\n",
        "                                transforms.ToTensor(),\n",
        "                                transforms.Normalize((0.5,0.5,0.5),\n",
        "                                                     (0.5,0.5,0.5))\n",
        "])\n",
        "transform2 = transforms.Compose([\n",
        "                                transforms.RandomCrop(32, padding=4),\n",
        "                                transforms.RandomHorizontalFlip(),\n",
        "                                transforms.ToTensor(),\n",
        "                                transforms.Normalize((0.5,0.5,0.5),\n",
        "                                                     (0.5,0.5,0.5))\n",
        "])\n",
        "train_dataset = datasets.ImageFolder(root=train_dir, transform=transform0)\n",
        "test_dataset = datasets.ImageFolder(root=test_dir, transform=transform0)\n",
        "\n",
        "# Hyperparameter\n",
        "\n",
        "\"\"\"\n",
        "추가로 실험해봐야 할 부분\n",
        "1. epoch / batch size / lr / stepsize 조정\n",
        "2. crop 조정\n",
        "\"\"\"\n",
        "\n",
        "EPOCHS = 50       # 40, 150, 300\n",
        "BATCH_SIZE = 8   # 16, 64, 128\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    dataset = train_dataset,\n",
        "    batch_size = BATCH_SIZE\n",
        ")\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    dataset = test_dataset,\n",
        "    batch_size = BATCH_SIZE\n",
        ")"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        },
        "id": "Uo0OEsxpuAnr",
        "outputId": "e2e01da7-86a4-404c-e231-7cd026c062ee"
      },
      "source": [
        "dataiter = iter(train_loader)\n",
        "images, labels = next(dataiter)\n",
        "img = utils.make_grid(images, padding=0)\n",
        "npimg = img.numpy()\n",
        "plt.figure(figsize = (10,7))\n",
        "plt.imshow(np.transpose(npimg,(1,2,0)))\n",
        "plt.show()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x504 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8VEjbj5_Uk5y",
        "outputId": "9d87133f-efe0-4e6e-dbf4-f9f135240444"
      },
      "source": [
        "img.shape"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 512, 4096])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5vE9HvQZm_8r",
        "outputId": "1fdbbe1a-8a32-4f54-b31f-52ded1037738"
      },
      "source": [
        "print(len(Labels))\n",
        "\"\"\"\n",
        "다른 데이터셋으로 실험 시 반드시 num_classes를 label의 수로 변경\n",
        "이미지 사이즈 조정 시 출력계층 고치기 self.linear = nn.Linear(64*49, num_classes)\n",
        "\"\"\"\n",
        "Labels"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['f-35 lightning', 'b-1 lancer']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CTlmqBMN7ksj"
      },
      "source": [
        "class BasicBlock(nn.Module):\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3,\n",
        "                               stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, planes,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, num_classes=2):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_planes = 16\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(16)\n",
        "        self.layer1 = self._make_layer(16, 2, stride=1)\n",
        "        self.layer2 = self._make_layer(32, 2, stride=2)\n",
        "        self.layer3 = self._make_layer(64, 2, stride=2)\n",
        "        self.linear = nn.Linear(64*256, num_classes)\n",
        "\n",
        "    def _make_layer(self, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(BasicBlock(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = F.avg_pool2d(out, 8)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C0SUiS5yyJI_"
      },
      "source": [
        "model = ResNet().to(DEVICE)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1,\n",
        "                      momentum=0.9, weight_decay=0.005)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SrODV4UeJ6uZ",
        "outputId": "71c5de99-ac57-456b-83b3-9b2acc37e3b5"
      },
      "source": [
        "print(model)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ResNet(\n",
            "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "  (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (layer1): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential(\n",
            "        (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential(\n",
            "        (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "  )\n",
            "  (linear): Linear(in_features=16384, out_features=2, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FU17_tx8Nfqc"
      },
      "source": [
        "def train(model, train_loader, optimizer, epoch):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(DEVICE), target.to(DEVICE)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.cross_entropy(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Exbxq_vAYqoc"
      },
      "source": [
        "def evaluate(model, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(DEVICE), target.to(DEVICE)\n",
        "            output = model(data)\n",
        "\n",
        "            # 배치 오차를 합산\n",
        "            test_loss += F.cross_entropy(output, target,\n",
        "                                         reduction='mean').item()\n",
        "\n",
        "            # 가장 높은 값을 가진 인덱스가 바로 예측값\n",
        "            pred = output.max(1, keepdim=True)[1]\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    test_accuracy = 100. * correct / len(test_loader.dataset)\n",
        "    return test_loss, test_accuracy"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQ_PTUoYUHjm"
      },
      "source": [
        "import gc\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-UT6KQyBE5zm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75f14650-5845-48bd-ff5b-4dc0db4bd5a9"
      },
      "source": [
        "for epoch in range(1, EPOCHS + 1):\n",
        "\n",
        "    train(model, train_loader, optimizer, epoch)\n",
        "    test_loss, test_accuracy = evaluate(model, test_loader)\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "    print('[{}] Test Loss: {:.4f}, Accuracy: {:.2f}%'.format(\n",
        "          epoch, test_loss, test_accuracy))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1] Test Loss: 4062478.0850, Accuracy: 13.00%\n",
            "[2] Test Loss: 11412052.8200, Accuracy: 13.00%\n",
            "[3] Test Loss: 92673270251.5200, Accuracy: 13.00%\n",
            "[4] Test Loss: 5768774.7100, Accuracy: 13.00%\n",
            "[5] Test Loss: 926219214520.3199, Accuracy: 13.00%\n",
            "[6] Test Loss: 127292637493455744.0000, Accuracy: 13.00%\n",
            "[7] Test Loss: 58493691922470664.0000, Accuracy: 13.00%\n",
            "[8] Test Loss: nan, Accuracy: 87.00%\n",
            "[9] Test Loss: nan, Accuracy: 87.00%\n",
            "[10] Test Loss: nan, Accuracy: 87.00%\n",
            "[11] Test Loss: nan, Accuracy: 87.00%\n",
            "[12] Test Loss: nan, Accuracy: 87.00%\n",
            "[13] Test Loss: nan, Accuracy: 87.00%\n",
            "[14] Test Loss: nan, Accuracy: 87.00%\n",
            "[15] Test Loss: nan, Accuracy: 87.00%\n",
            "[16] Test Loss: nan, Accuracy: 87.00%\n",
            "[17] Test Loss: nan, Accuracy: 87.00%\n",
            "[18] Test Loss: nan, Accuracy: 87.00%\n",
            "[19] Test Loss: nan, Accuracy: 87.00%\n",
            "[20] Test Loss: nan, Accuracy: 87.00%\n",
            "[21] Test Loss: nan, Accuracy: 87.00%\n",
            "[22] Test Loss: nan, Accuracy: 87.00%\n",
            "[23] Test Loss: nan, Accuracy: 87.00%\n",
            "[24] Test Loss: nan, Accuracy: 87.00%\n",
            "[25] Test Loss: nan, Accuracy: 87.00%\n",
            "[26] Test Loss: nan, Accuracy: 87.00%\n",
            "[27] Test Loss: nan, Accuracy: 87.00%\n",
            "[28] Test Loss: nan, Accuracy: 87.00%\n",
            "[29] Test Loss: nan, Accuracy: 87.00%\n",
            "[30] Test Loss: nan, Accuracy: 87.00%\n",
            "[31] Test Loss: nan, Accuracy: 87.00%\n",
            "[32] Test Loss: nan, Accuracy: 87.00%\n",
            "[33] Test Loss: nan, Accuracy: 87.00%\n",
            "[34] Test Loss: nan, Accuracy: 87.00%\n",
            "[35] Test Loss: nan, Accuracy: 87.00%\n",
            "[36] Test Loss: nan, Accuracy: 87.00%\n",
            "[37] Test Loss: nan, Accuracy: 87.00%\n",
            "[38] Test Loss: nan, Accuracy: 87.00%\n",
            "[39] Test Loss: nan, Accuracy: 87.00%\n",
            "[40] Test Loss: nan, Accuracy: 87.00%\n",
            "[41] Test Loss: nan, Accuracy: 87.00%\n",
            "[42] Test Loss: nan, Accuracy: 87.00%\n",
            "[43] Test Loss: nan, Accuracy: 87.00%\n",
            "[44] Test Loss: nan, Accuracy: 87.00%\n",
            "[45] Test Loss: nan, Accuracy: 87.00%\n",
            "[46] Test Loss: nan, Accuracy: 87.00%\n",
            "[47] Test Loss: nan, Accuracy: 87.00%\n",
            "[48] Test Loss: nan, Accuracy: 87.00%\n",
            "[49] Test Loss: nan, Accuracy: 87.00%\n",
            "[50] Test Loss: nan, Accuracy: 87.00%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6IWBtfOqkNh"
      },
      "source": [
        "## 신경망 검사"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GosNa7uWp3Qf"
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "\n",
        "dataiter = iter(test_loader)\n",
        "images, labels = dataiter.next()"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5RUa3GwuiXO"
      },
      "source": [
        "## 가중치, 파라미터 확인"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DTTiSc4ezc9r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b67ded5d-a0dc-4f5b-c7d4-2d0b035dd316"
      },
      "source": [
        "print(\"Model's state dict:\")\n",
        "for param_tensor in model.state_dict():\n",
        "  print(param_tensor, '\\t', model.state_dict()[param_tensor].size())"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model's state dict:\n",
            "conv1.weight \t torch.Size([16, 3, 3, 3])\n",
            "bn1.weight \t torch.Size([16])\n",
            "bn1.bias \t torch.Size([16])\n",
            "bn1.running_mean \t torch.Size([16])\n",
            "bn1.running_var \t torch.Size([16])\n",
            "bn1.num_batches_tracked \t torch.Size([])\n",
            "layer1.0.conv1.weight \t torch.Size([16, 16, 3, 3])\n",
            "layer1.0.bn1.weight \t torch.Size([16])\n",
            "layer1.0.bn1.bias \t torch.Size([16])\n",
            "layer1.0.bn1.running_mean \t torch.Size([16])\n",
            "layer1.0.bn1.running_var \t torch.Size([16])\n",
            "layer1.0.bn1.num_batches_tracked \t torch.Size([])\n",
            "layer1.0.conv2.weight \t torch.Size([16, 16, 3, 3])\n",
            "layer1.0.bn2.weight \t torch.Size([16])\n",
            "layer1.0.bn2.bias \t torch.Size([16])\n",
            "layer1.0.bn2.running_mean \t torch.Size([16])\n",
            "layer1.0.bn2.running_var \t torch.Size([16])\n",
            "layer1.0.bn2.num_batches_tracked \t torch.Size([])\n",
            "layer1.1.conv1.weight \t torch.Size([16, 16, 3, 3])\n",
            "layer1.1.bn1.weight \t torch.Size([16])\n",
            "layer1.1.bn1.bias \t torch.Size([16])\n",
            "layer1.1.bn1.running_mean \t torch.Size([16])\n",
            "layer1.1.bn1.running_var \t torch.Size([16])\n",
            "layer1.1.bn1.num_batches_tracked \t torch.Size([])\n",
            "layer1.1.conv2.weight \t torch.Size([16, 16, 3, 3])\n",
            "layer1.1.bn2.weight \t torch.Size([16])\n",
            "layer1.1.bn2.bias \t torch.Size([16])\n",
            "layer1.1.bn2.running_mean \t torch.Size([16])\n",
            "layer1.1.bn2.running_var \t torch.Size([16])\n",
            "layer1.1.bn2.num_batches_tracked \t torch.Size([])\n",
            "layer2.0.conv1.weight \t torch.Size([32, 16, 3, 3])\n",
            "layer2.0.bn1.weight \t torch.Size([32])\n",
            "layer2.0.bn1.bias \t torch.Size([32])\n",
            "layer2.0.bn1.running_mean \t torch.Size([32])\n",
            "layer2.0.bn1.running_var \t torch.Size([32])\n",
            "layer2.0.bn1.num_batches_tracked \t torch.Size([])\n",
            "layer2.0.conv2.weight \t torch.Size([32, 32, 3, 3])\n",
            "layer2.0.bn2.weight \t torch.Size([32])\n",
            "layer2.0.bn2.bias \t torch.Size([32])\n",
            "layer2.0.bn2.running_mean \t torch.Size([32])\n",
            "layer2.0.bn2.running_var \t torch.Size([32])\n",
            "layer2.0.bn2.num_batches_tracked \t torch.Size([])\n",
            "layer2.0.shortcut.0.weight \t torch.Size([32, 16, 1, 1])\n",
            "layer2.0.shortcut.1.weight \t torch.Size([32])\n",
            "layer2.0.shortcut.1.bias \t torch.Size([32])\n",
            "layer2.0.shortcut.1.running_mean \t torch.Size([32])\n",
            "layer2.0.shortcut.1.running_var \t torch.Size([32])\n",
            "layer2.0.shortcut.1.num_batches_tracked \t torch.Size([])\n",
            "layer2.1.conv1.weight \t torch.Size([32, 32, 3, 3])\n",
            "layer2.1.bn1.weight \t torch.Size([32])\n",
            "layer2.1.bn1.bias \t torch.Size([32])\n",
            "layer2.1.bn1.running_mean \t torch.Size([32])\n",
            "layer2.1.bn1.running_var \t torch.Size([32])\n",
            "layer2.1.bn1.num_batches_tracked \t torch.Size([])\n",
            "layer2.1.conv2.weight \t torch.Size([32, 32, 3, 3])\n",
            "layer2.1.bn2.weight \t torch.Size([32])\n",
            "layer2.1.bn2.bias \t torch.Size([32])\n",
            "layer2.1.bn2.running_mean \t torch.Size([32])\n",
            "layer2.1.bn2.running_var \t torch.Size([32])\n",
            "layer2.1.bn2.num_batches_tracked \t torch.Size([])\n",
            "layer3.0.conv1.weight \t torch.Size([64, 32, 3, 3])\n",
            "layer3.0.bn1.weight \t torch.Size([64])\n",
            "layer3.0.bn1.bias \t torch.Size([64])\n",
            "layer3.0.bn1.running_mean \t torch.Size([64])\n",
            "layer3.0.bn1.running_var \t torch.Size([64])\n",
            "layer3.0.bn1.num_batches_tracked \t torch.Size([])\n",
            "layer3.0.conv2.weight \t torch.Size([64, 64, 3, 3])\n",
            "layer3.0.bn2.weight \t torch.Size([64])\n",
            "layer3.0.bn2.bias \t torch.Size([64])\n",
            "layer3.0.bn2.running_mean \t torch.Size([64])\n",
            "layer3.0.bn2.running_var \t torch.Size([64])\n",
            "layer3.0.bn2.num_batches_tracked \t torch.Size([])\n",
            "layer3.0.shortcut.0.weight \t torch.Size([64, 32, 1, 1])\n",
            "layer3.0.shortcut.1.weight \t torch.Size([64])\n",
            "layer3.0.shortcut.1.bias \t torch.Size([64])\n",
            "layer3.0.shortcut.1.running_mean \t torch.Size([64])\n",
            "layer3.0.shortcut.1.running_var \t torch.Size([64])\n",
            "layer3.0.shortcut.1.num_batches_tracked \t torch.Size([])\n",
            "layer3.1.conv1.weight \t torch.Size([64, 64, 3, 3])\n",
            "layer3.1.bn1.weight \t torch.Size([64])\n",
            "layer3.1.bn1.bias \t torch.Size([64])\n",
            "layer3.1.bn1.running_mean \t torch.Size([64])\n",
            "layer3.1.bn1.running_var \t torch.Size([64])\n",
            "layer3.1.bn1.num_batches_tracked \t torch.Size([])\n",
            "layer3.1.conv2.weight \t torch.Size([64, 64, 3, 3])\n",
            "layer3.1.bn2.weight \t torch.Size([64])\n",
            "layer3.1.bn2.bias \t torch.Size([64])\n",
            "layer3.1.bn2.running_mean \t torch.Size([64])\n",
            "layer3.1.bn2.running_var \t torch.Size([64])\n",
            "layer3.1.bn2.num_batches_tracked \t torch.Size([])\n",
            "linear.weight \t torch.Size([2, 16384])\n",
            "linear.bias \t torch.Size([2])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uootomSzl6FN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed0cd296-a70b-47b9-bafc-a3df28cf2363"
      },
      "source": [
        "print(\"Optimizer's state_dict:\")\n",
        "for var_name in optimizer.state_dict():\n",
        "  print(var_name, '\\t', optimizer.state_dict()[var_name])"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Optimizer's state_dict:\n",
            "state \t {0: {'momentum_buffer': tensor([[[[        nan,         nan,         nan],\n",
            "          [        nan,         nan,         nan],\n",
            "          [        nan,         nan,         nan]],\n",
            "\n",
            "         [[        nan,         nan,         nan],\n",
            "          [        nan,         nan,         nan],\n",
            "          [        nan,         nan,         nan]],\n",
            "\n",
            "         [[        nan,         nan,         nan],\n",
            "          [        nan,         nan,         nan],\n",
            "          [        nan,         nan,         nan]]],\n",
            "\n",
            "\n",
            "        [[[        nan,         nan,         nan],\n",
            "          [        nan,         nan,         nan],\n",
            "          [        nan,         nan,         nan]],\n",
            "\n",
            "         [[        nan,         nan,         nan],\n",
            "          [        nan,         nan,         nan],\n",
            "          [        nan,         nan,         nan]],\n",
            "\n",
            "         [[        nan,         nan,         nan],\n",
            "          [        nan,         nan,         nan],\n",
            "          [        nan,         nan,         nan]]],\n",
            "\n",
            "\n",
            "        [[[        nan,         nan,         nan],\n",
            "          [        nan,         nan,         nan],\n",
            "          [        nan,         nan,         nan]],\n",
            "\n",
            "         [[        nan,         nan,         nan],\n",
            "          [        nan,         nan,         nan],\n",
            "          [        nan,         nan,         nan]],\n",
            "\n",
            "         [[        nan,         nan,         nan],\n",
            "          [        nan,         nan,         nan],\n",
            "          [        nan,         nan,         nan]]],\n",
            "\n",
            "\n",
            "        [[[-1.1855e+02, -1.1437e+02, -1.1693e+02],\n",
            "          [-2.5602e+02, -2.5182e+02, -2.5549e+02],\n",
            "          [-2.0685e+02, -2.0138e+02, -2.0464e+02]],\n",
            "\n",
            "         [[-1.1855e+02, -1.1437e+02, -1.1693e+02],\n",
            "          [-2.5602e+02, -2.5182e+02, -2.5549e+02],\n",
            "          [-2.0685e+02, -2.0138e+02, -2.0464e+02]],\n",
            "\n",
            "         [[-1.1855e+02, -1.1437e+02, -1.1693e+02],\n",
            "          [-2.5602e+02, -2.5182e+02, -2.5549e+02],\n",
            "          [-2.0685e+02, -2.0138e+02, -2.0464e+02]]],\n",
            "\n",
            "\n",
            "        [[[ 7.2239e-02,  9.2534e-02, -2.9704e-02],\n",
            "          [ 1.4426e-02,  4.7756e-02, -6.4037e-02],\n",
            "          [-1.7188e-01, -1.2773e-01, -2.2071e-01]],\n",
            "\n",
            "         [[ 7.2025e-02,  9.2403e-02, -3.0125e-02],\n",
            "          [ 1.3740e-02,  4.7851e-02, -6.2945e-02],\n",
            "          [-1.7119e-01, -1.2776e-01, -2.2019e-01]],\n",
            "\n",
            "         [[ 7.2846e-02,  9.2389e-02, -2.9096e-02],\n",
            "          [ 1.4302e-02,  4.8026e-02, -6.3957e-02],\n",
            "          [-1.7174e-01, -1.2763e-01, -2.1990e-01]]],\n",
            "\n",
            "\n",
            "        [[[        nan,         nan,         nan],\n",
            "          [        nan,         nan,         nan],\n",
            "          [        nan,         nan,         nan]],\n",
            "\n",
            "         [[        nan,         nan,         nan],\n",
            "          [        nan,         nan,         nan],\n",
            "          [        nan,         nan,         nan]],\n",
            "\n",
            "         [[        nan,         nan,         nan],\n",
            "          [        nan,         nan,         nan],\n",
            "          [        nan,         nan,         nan]]],\n",
            "\n",
            "\n",
            "        [[[ 8.1303e+03,  8.4003e+03,  9.9206e+03],\n",
            "          [ 3.3940e+03,  3.8879e+03,  5.1946e+03],\n",
            "          [-2.7085e+03, -1.6085e+03, -7.3773e+02]],\n",
            "\n",
            "         [[ 8.1306e+03,  8.4005e+03,  9.9206e+03],\n",
            "          [ 3.3939e+03,  3.8879e+03,  5.1946e+03],\n",
            "          [-2.7086e+03, -1.6084e+03, -7.3784e+02]],\n",
            "\n",
            "         [[ 8.1304e+03,  8.4003e+03,  9.9205e+03],\n",
            "          [ 3.3937e+03,  3.8877e+03,  5.1946e+03],\n",
            "          [-2.7086e+03, -1.6085e+03, -7.3766e+02]]],\n",
            "\n",
            "\n",
            "        [[[ 2.2662e+00,  1.0643e+00,  1.8189e+00],\n",
            "          [-1.0487e+01, -1.1396e+01, -1.1254e+01],\n",
            "          [ 1.7203e+00,  7.5041e-01, -5.9654e-01]],\n",
            "\n",
            "         [[ 2.2661e+00,  1.0644e+00,  1.8180e+00],\n",
            "          [-1.0487e+01, -1.1396e+01, -1.1253e+01],\n",
            "          [ 1.7206e+00,  7.5112e-01, -5.9582e-01]],\n",
            "\n",
            "         [[ 2.2656e+00,  1.0651e+00,  1.8181e+00],\n",
            "          [-1.0487e+01, -1.1396e+01, -1.1254e+01],\n",
            "          [ 1.7211e+00,  7.5070e-01, -5.9613e-01]]],\n",
            "\n",
            "\n",
            "        [[[ 2.0487e-02,  2.1025e-02,  1.6435e-02],\n",
            "          [ 1.7148e-02,  1.5795e-02,  1.0985e-02],\n",
            "          [ 2.2139e-02,  1.8101e-02,  1.2134e-02]],\n",
            "\n",
            "         [[ 2.0233e-02,  2.0295e-02,  1.6775e-02],\n",
            "          [ 1.7196e-02,  1.5650e-02,  1.0490e-02],\n",
            "          [ 2.2682e-02,  1.8340e-02,  1.1309e-02]],\n",
            "\n",
            "         [[ 2.0471e-02,  2.0234e-02,  1.6965e-02],\n",
            "          [ 1.7153e-02,  1.6247e-02,  1.0288e-02],\n",
            "          [ 2.2081e-02,  1.8116e-02,  1.1554e-02]]],\n",
            "\n",
            "\n",
            "        [[[        nan,         nan,         nan],\n",
            "          [        nan,         nan,         nan],\n",
            "          [        nan,         nan,         nan]],\n",
            "\n",
            "         [[        nan,         nan,         nan],\n",
            "          [        nan,         nan,         nan],\n",
            "          [        nan,         nan,         nan]],\n",
            "\n",
            "         [[        nan,         nan,         nan],\n",
            "          [        nan,         nan,         nan],\n",
            "          [        nan,         nan,         nan]]],\n",
            "\n",
            "\n",
            "        [[[-1.5792e+01, -1.4128e+01, -5.6432e+00],\n",
            "          [-8.2351e+01, -7.8927e+01, -6.7523e+01],\n",
            "          [-9.4078e+01, -9.1368e+01, -8.0244e+01]],\n",
            "\n",
            "         [[-1.5792e+01, -1.4127e+01, -5.6424e+00],\n",
            "          [-8.2352e+01, -7.8927e+01, -6.7522e+01],\n",
            "          [-9.4078e+01, -9.1368e+01, -8.0244e+01]],\n",
            "\n",
            "         [[-1.5793e+01, -1.4127e+01, -5.6425e+00],\n",
            "          [-8.2351e+01, -7.8927e+01, -6.7522e+01],\n",
            "          [-9.4078e+01, -9.1368e+01, -8.0244e+01]]],\n",
            "\n",
            "\n",
            "        [[[        nan,         nan,         nan],\n",
            "          [        nan,         nan,         nan],\n",
            "          [        nan,         nan,         nan]],\n",
            "\n",
            "         [[        nan,         nan,         nan],\n",
            "          [        nan,         nan,         nan],\n",
            "          [        nan,         nan,         nan]],\n",
            "\n",
            "         [[        nan,         nan,         nan],\n",
            "          [        nan,         nan,         nan],\n",
            "          [        nan,         nan,         nan]]],\n",
            "\n",
            "\n",
            "        [[[        nan,         nan,         nan],\n",
            "          [        nan,         nan,         nan],\n",
            "          [        nan,         nan,         nan]],\n",
            "\n",
            "         [[        nan,         nan,         nan],\n",
            "          [        nan,         nan,         nan],\n",
            "          [        nan,         nan,         nan]],\n",
            "\n",
            "         [[        nan,         nan,         nan],\n",
            "          [        nan,         nan,         nan],\n",
            "          [        nan,         nan,         nan]]],\n",
            "\n",
            "\n",
            "        [[[ 4.5107e-02,  4.8581e-02,  5.8774e-03],\n",
            "          [-2.4861e-02, -1.9578e-02, -5.6736e-02],\n",
            "          [-1.2836e-01, -1.2041e-01, -1.4828e-01]],\n",
            "\n",
            "         [[ 4.5113e-02,  4.8294e-02,  5.0590e-03],\n",
            "          [-2.4804e-02, -2.0000e-02, -5.7407e-02],\n",
            "          [-1.2902e-01, -1.2032e-01, -1.4851e-01]],\n",
            "\n",
            "         [[ 4.4989e-02,  4.7601e-02,  5.1690e-03],\n",
            "          [-2.5539e-02, -2.0116e-02, -5.6937e-02],\n",
            "          [-1.2882e-01, -1.2061e-01, -1.4930e-01]]],\n",
            "\n",
            "\n",
            "        [[[-3.6189e-01, -3.5097e-01, -7.6515e-01],\n",
            "          [-5.3708e-01, -4.8201e-01, -8.5059e-01],\n",
            "          [-9.0920e-01, -8.1938e-01, -1.1314e+00]],\n",
            "\n",
            "         [[-3.6200e-01, -3.5169e-01, -7.6480e-01],\n",
            "          [-5.3652e-01, -4.8104e-01, -8.5109e-01],\n",
            "          [-9.0857e-01, -8.1925e-01, -1.1307e+00]],\n",
            "\n",
            "         [[-3.6230e-01, -3.5145e-01, -7.6547e-01],\n",
            "          [-5.3641e-01, -4.8114e-01, -8.5084e-01],\n",
            "          [-9.0845e-01, -8.1927e-01, -1.1308e+00]]],\n",
            "\n",
            "\n",
            "        [[[ 2.3188e-02,  1.3355e-02,  9.6716e-03],\n",
            "          [ 1.0858e-02,  2.4297e-03,  3.0663e-03],\n",
            "          [ 1.9852e-02,  1.4357e-02,  1.8784e-02]],\n",
            "\n",
            "         [[ 2.2908e-02,  1.3863e-02,  9.0902e-03],\n",
            "          [ 1.0605e-02,  2.2423e-03,  3.0242e-03],\n",
            "          [ 1.9943e-02,  1.3842e-02,  1.8497e-02]],\n",
            "\n",
            "         [[ 2.2927e-02,  1.3849e-02,  9.7642e-03],\n",
            "          [ 1.1039e-02,  3.0519e-03,  2.8026e-03],\n",
            "          [ 1.9162e-02,  1.4078e-02,  1.8127e-02]]]], device='cuda:0')}, 1: {'momentum_buffer': tensor([        nan,         nan,         nan, -6.4668e+01, -1.6167e-01,\n",
            "                nan, -1.9878e+03, -4.1529e+00,  2.7066e-03,         nan,\n",
            "        -5.5992e+01,         nan,         nan, -7.0835e-02, -1.2880e-02,\n",
            "        -9.2250e-03], device='cuda:0')}, 2: {'momentum_buffer': tensor([        nan,         nan,         nan, -1.6601e+02, -4.2217e-01,\n",
            "                nan, -1.3349e+04, -5.3066e+00, -1.2769e-01,         nan,\n",
            "        -9.1957e+01,         nan,         nan, -2.2586e-01, -1.5428e-01,\n",
            "        -2.9107e-01], device='cuda:0')}, 3: {'momentum_buffer': tensor([[[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]]], device='cuda:0')}, 4: {'momentum_buffer': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
            "       device='cuda:0')}, 5: {'momentum_buffer': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
            "       device='cuda:0')}, 6: {'momentum_buffer': tensor([[[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]]], device='cuda:0')}, 7: {'momentum_buffer': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
            "       device='cuda:0')}, 8: {'momentum_buffer': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
            "       device='cuda:0')}, 9: {'momentum_buffer': tensor([[[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]]], device='cuda:0')}, 10: {'momentum_buffer': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
            "       device='cuda:0')}, 11: {'momentum_buffer': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
            "       device='cuda:0')}, 12: {'momentum_buffer': tensor([[[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]]], device='cuda:0')}, 13: {'momentum_buffer': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
            "       device='cuda:0')}, 14: {'momentum_buffer': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
            "       device='cuda:0')}, 15: {'momentum_buffer': tensor([[[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]]], device='cuda:0')}, 16: {'momentum_buffer': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0')}, 17: {'momentum_buffer': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0')}, 18: {'momentum_buffer': tensor([[[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]]], device='cuda:0')}, 19: {'momentum_buffer': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0')}, 20: {'momentum_buffer': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0')}, 21: {'momentum_buffer': tensor([[[[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]]],\n",
            "\n",
            "\n",
            "        [[[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]]],\n",
            "\n",
            "\n",
            "        [[[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]]],\n",
            "\n",
            "\n",
            "        [[[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]]],\n",
            "\n",
            "\n",
            "        [[[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]]],\n",
            "\n",
            "\n",
            "        [[[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]]],\n",
            "\n",
            "\n",
            "        [[[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]]],\n",
            "\n",
            "\n",
            "        [[[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]]],\n",
            "\n",
            "\n",
            "        [[[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]]],\n",
            "\n",
            "\n",
            "        [[[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]]],\n",
            "\n",
            "\n",
            "        [[[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]]],\n",
            "\n",
            "\n",
            "        [[[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]]],\n",
            "\n",
            "\n",
            "        [[[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]]],\n",
            "\n",
            "\n",
            "        [[[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]]],\n",
            "\n",
            "\n",
            "        [[[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]]],\n",
            "\n",
            "\n",
            "        [[[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]]],\n",
            "\n",
            "\n",
            "        [[[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]]],\n",
            "\n",
            "\n",
            "        [[[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]]],\n",
            "\n",
            "\n",
            "        [[[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]]],\n",
            "\n",
            "\n",
            "        [[[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]]],\n",
            "\n",
            "\n",
            "        [[[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]]],\n",
            "\n",
            "\n",
            "        [[[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]]],\n",
            "\n",
            "\n",
            "        [[[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]]],\n",
            "\n",
            "\n",
            "        [[[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]]],\n",
            "\n",
            "\n",
            "        [[[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]]],\n",
            "\n",
            "\n",
            "        [[[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]]],\n",
            "\n",
            "\n",
            "        [[[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]]],\n",
            "\n",
            "\n",
            "        [[[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]]],\n",
            "\n",
            "\n",
            "        [[[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]]],\n",
            "\n",
            "\n",
            "        [[[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]]],\n",
            "\n",
            "\n",
            "        [[[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]]],\n",
            "\n",
            "\n",
            "        [[[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]]]], device='cuda:0')}, 22: {'momentum_buffer': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0')}, 23: {'momentum_buffer': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0')}, 24: {'momentum_buffer': tensor([[[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]]], device='cuda:0')}, 25: {'momentum_buffer': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0')}, 26: {'momentum_buffer': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0')}, 27: {'momentum_buffer': tensor([[[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]]], device='cuda:0')}, 28: {'momentum_buffer': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0')}, 29: {'momentum_buffer': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0')}, 30: {'momentum_buffer': tensor([[[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]]], device='cuda:0')}, 31: {'momentum_buffer': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
            "       device='cuda:0')}, 32: {'momentum_buffer': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
            "       device='cuda:0')}, 33: {'momentum_buffer': tensor([[[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]]], device='cuda:0')}, 34: {'momentum_buffer': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
            "       device='cuda:0')}, 35: {'momentum_buffer': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
            "       device='cuda:0')}, 36: {'momentum_buffer': tensor([[[[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]]],\n",
            "\n",
            "\n",
            "        [[[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]]],\n",
            "\n",
            "\n",
            "        [[[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]]],\n",
            "\n",
            "\n",
            "        [[[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]]],\n",
            "\n",
            "\n",
            "        [[[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]]]], device='cuda:0')}, 37: {'momentum_buffer': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
            "       device='cuda:0')}, 38: {'momentum_buffer': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
            "       device='cuda:0')}, 39: {'momentum_buffer': tensor([[[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]]], device='cuda:0')}, 40: {'momentum_buffer': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
            "       device='cuda:0')}, 41: {'momentum_buffer': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
            "       device='cuda:0')}, 42: {'momentum_buffer': tensor([[[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]]], device='cuda:0')}, 43: {'momentum_buffer': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
            "       device='cuda:0')}, 44: {'momentum_buffer': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
            "       device='cuda:0')}, 45: {'momentum_buffer': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
            "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0')}, 46: {'momentum_buffer': tensor([nan, nan], device='cuda:0')}}\n",
            "param_groups \t [{'lr': 1.0000000000000004e-06, 'momentum': 0.9, 'dampening': 0, 'weight_decay': 0.005, 'nesterov': False, 'initial_lr': 0.1, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46]}]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6AgPoA0u18d"
      },
      "source": [
        "## 모델 저장"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hd32m4Gy6Ua0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "db736257-3d1c-474a-dfd2-45a237c10200"
      },
      "source": [
        "# save model\n",
        "\n",
        "PATH = './model_v0.04.pth'\n",
        "torch.save(model.state_dict(), PATH)\n",
        "\"\"\"\n",
        "불러올 땐\n",
        "model = TheModelClass(*args, **kwargs)\n",
        "model.load_state_dict(torch.load(PATH))\n",
        "model.eval()\n",
        "\"\"\""
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n불러올 땐\\nmodel = TheModelClass(*args, **kwargs)\\nmodel.load_state_dict(torch.load(PATH))\\nmodel.eval()\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JdR4jNelXBNR"
      },
      "source": [
        "# construct model on cuda if available\n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "\n",
        "class CNNClassifier(nn.Module):\n",
        "    \n",
        "    def __init__(self):\n",
        "        # 항상 torch.nn.Module을 상속받고 시작\n",
        "        super(CNNClassifier, self).__init__()\n",
        "        conv1 = nn.Conv2d(1, 6, 5, 1) # 6@24*24\n",
        "        # activation ReLU\n",
        "        pool1 = nn.MaxPool2d(2) # 6@12*12\n",
        "        conv2 = nn.Conv2d(6, 16, 5, 1) # 16@8*8\n",
        "        # activation ReLU\n",
        "        pool2 = nn.MaxPool2d(2) # 16@4*4\n",
        "        \n",
        "        self.conv_module = nn.Sequential(\n",
        "            conv1,\n",
        "            nn.ReLU(),\n",
        "            pool1,\n",
        "            conv2,\n",
        "            nn.ReLU(),\n",
        "            pool2\n",
        "        )\n",
        "        \n",
        "        fc1 = nn.Linear(32768, 1024)\n",
        "        # activation ReLU\n",
        "        fc2 = nn.Linear(1024, 64)\n",
        "        # activation ReLU\n",
        "        fc3 = nn.Linear(64, 10)\n",
        "\n",
        "        self.fc_module = nn.Sequential(\n",
        "            fc1,\n",
        "            nn.ReLU(),\n",
        "            fc2,\n",
        "            nn.ReLU(),\n",
        "            fc3\n",
        "        )\n",
        "        \n",
        "        # gpu로 할당\n",
        "        if use_cuda:\n",
        "            self.conv_module = self.conv_module.cuda()\n",
        "            self.fc_module = self.fc_module.cuda()\n",
        "        \n",
        "    def forward(self, x):\n",
        "        out = self.conv_module(x) # @16*4*4\n",
        "        # make linear\n",
        "        dim = 1\n",
        "        for d in out.size()[1:]: #16, 4, 4\n",
        "            dim = dim * d\n",
        "        out = out.view(-1, dim)\n",
        "        out = self.fc_module(out)\n",
        "        return F.softmax(out, dim=1)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6lvBD-IjY_1y"
      },
      "source": [
        "model2 = CNNClassifier()"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZ6eAxMpZBhj"
      },
      "source": [
        "learning_rate = 1e-3\n",
        "optimizer = optim.Adam(model2.parameters(), lr=learning_rate)"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VvrBAEF1ZB2O",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 749
        },
        "outputId": "a53ba95b-f53e-4361-b487-5eee98d0a13a"
      },
      "source": [
        "# create figure for plotting\n",
        "import itertools\n",
        "row_num = 2\n",
        "col_num = 4\n",
        "fig, ax = plt.subplots(row_num, col_num, figsize=(6,6))\n",
        "for i, j in itertools.product(range(row_num), range(col_num)):\n",
        "    ax[i,j].get_xaxis().set_visible(False)\n",
        "    ax[i,j].get_yaxis().set_visible(False) \n",
        "    \n",
        "trn_loss_list = []\n",
        "val_loss_list = []\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    trn_loss = 0.0\n",
        "    for i, data in enumerate(train_loader):\n",
        "        x, label = data\n",
        "        if use_cuda:\n",
        "            x = x.cuda()\n",
        "            label = label.cuda()\n",
        "        # grad init\n",
        "        optimizer.zero_grad()\n",
        "        # forward propagation\n",
        "        model_output = model2(x)\n",
        "        # calculate loss\n",
        "        loss = criterion(model_output, label)\n",
        "        # back propagation \n",
        "        loss.backward()\n",
        "        # weight update\n",
        "        optimizer.step()\n",
        "        \n",
        "        # trn_loss summary\n",
        "        train_loss += loss.item()\n",
        "        # del (memory issue)\n",
        "        del loss\n",
        "        del model_output\n",
        "        \n",
        "        # 학습과정 출력\n",
        "        if (i+1) % 100 == 0: # every 100 mini-batches\n",
        "            with torch.no_grad(): # very very very very important!!!\n",
        "                test_loss = 0.0\n",
        "                for j, val in enumerate(test_loader):\n",
        "                    test_x, test_label = test\n",
        "                    if use_cuda:\n",
        "                        test_x = test_x.cuda()\n",
        "                        test_label = test_label.cuda()\n",
        "                    test_output = cnn(val_x)\n",
        "                    t_loss = criterion(test_output, test_label)\n",
        "                    test_loss += t_loss\n",
        "\n",
        "            # draw last val dataset\n",
        "            for k in range(row_num*col_num):\n",
        "                ii = k//col_num\n",
        "                jj = k%col_num\n",
        "                ax[ii,jj].cla() # clear the current axis\n",
        "                ax[ii,jj].imshow(val_x[k,:].data.cpu().numpy().reshape(28,28), cmap='Greys')\n",
        "            \n",
        "            display.clear_output(wait=True)\n",
        "            display.display(plt.gcf()) # get a reference to a current figure\n",
        "                \n",
        "            print(\"label: {}\".format(val_label[:row_num*col_num]))\n",
        "            print(\"prediction: {}\".format(val_output.argmax(dim=1)[:row_num*col_num]))\n",
        "            del val_output\n",
        "            del v_loss\n",
        "            \n",
        "            print(\"epoch: {}/{} | step: {}/{} | trn loss: {:.4f} | val loss: {:.4f}\".format(\n",
        "                epoch+1, num_epochs, i+1, num_batches, trn_loss / 100, val_loss / len(val_loader)\n",
        "            ))            \n",
        "            \n",
        "            train_loss_list.append(train_loss/100)\n",
        "            test_loss_list.append(test_loss/len(test_loader))\n",
        "            train_loss = 0.0"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-43-af70787aca91>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m# forward propagation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mmodel_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0;31m# calculate loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-36-f7c07f0b28c7>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# @16*4*4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m         \u001b[0;31m# make linear\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 443\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    438\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    439\u001b[0m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0;32m--> 440\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [6, 1, 5, 5], expected input[8, 3, 512, 512] to have 1 channels, but got 3 channels instead"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x432 with 8 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJ0EptHBZB-j"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nXOMWNpOZCFO"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s5kS454XZCK7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g0g7DICNZCQb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZkJ35KctZCVz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "14OyExMFXJqU"
      },
      "source": [
        "model2 = ResNet2().to(DEVICE)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1,\n",
        "                      momentum=0.9, weight_decay=0.005)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}