{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "CNN Classification v0.04.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNrMQPAUHNpEBtvJmykwIOP",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chanwoo522/Hackathon/blob/main/CNN_Classification_v0_06.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_7ExL9mBFv8"
      },
      "source": [
        "## 라이브러리 임포트"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3EJ9vJD8RaD"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torchvision import transforms, datasets, utils\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "import re\n",
        "import shutil"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OPIaATPUIzYU"
      },
      "source": [
        "USE_CUDA = torch.cuda.is_available()\n",
        "DEVICE = torch.device(\"cuda\" if USE_CUDA else \"cpu\")"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YjFu_3-tBSZ3"
      },
      "source": [
        "## 구글 드라이브 마운트"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pK5FN0GbN1qj",
        "outputId": "65e4ada3-75bd-4dbe-e747-5f9bf1a639d8"
      },
      "source": [
        "# load image files\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "boFZ0eN1BQeI"
      },
      "source": [
        "## 작업 폴더 설정"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ocIf9WqpDssw"
      },
      "source": [
        "## directory 설정\n",
        "cur_dir = os.path.abspath('/content/drive/Shareddrives/aircraft')\n",
        "image_dir = os.path.join(cur_dir, 'edge')\n",
        "image_files = [fname for fname in os.listdir(image_dir) if os.path.splitext(fname)[-1] == '.png']"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OwbgFf6Z8Xoi",
        "outputId": "9ff230dd-405c-485f-b48e-62b05149d9f3"
      },
      "source": [
        "# labeling\n",
        "\n",
        "Labels = set()\n",
        "\n",
        "for image_file in image_files:\n",
        "    file_name = os.path.splitext(image_file)[0]\n",
        "    class_name = re.sub('_\\d+', '', file_name)\n",
        "    Labels.add(class_name)\n",
        "Labels = list(Labels)\n",
        "\n",
        "# ['j10','j11','j15','j16','j20','j31','JL10','j6',\n",
        "#  'y8g','y9jb','y20','kj2000','bjk005','ch3','wingloong',\n",
        "#  'xianglong','z9','z18','mig31','su24','su27','su30',\n",
        "#  'su35','su57','tu95ms','tu142','a50','il38','il20',\n",
        "#  'f2','e767','ec1','ch47j','p1','f4','f5','fa50','f15',\n",
        "#  'f16','fa18','f22','f35','a10','b1','b2','c130','p3',\n",
        "#  'p8','rc135','e737','kc330','u2v']\n",
        "print(Labels)\n",
        "img_size = 512"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['b-1 lancer', 'f-35 lightning']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EbGrBPR3bnEr"
      },
      "source": [
        "## Experiment directory setting\n",
        "\n",
        "train_dir = os.path.join(cur_dir, 'train_dir')\n",
        "test_dir = os.path.join(cur_dir, 'test_dir')\n",
        "\n",
        "os.makedirs(train_dir, exist_ok=True)\n",
        "os.makedirs(test_dir, exist_ok=True)\n",
        "\n",
        "for label in Labels:\n",
        "  label_dir = os.path.join(train_dir, label)\n",
        "  os.makedirs(label_dir, exist_ok=True)\n",
        "  label_dir = os.path.join(test_dir, label)\n",
        "  os.makedirs(label_dir, exist_ok=True)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RdjznjFJCIHo"
      },
      "source": [
        "## 이미지 파일 train, test data로 분리"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-axG8zDBp52"
      },
      "source": [
        "# practice file separate\n",
        "cnt = 0\n",
        "previous_class = \"\"\n",
        "image_files.sort()\n",
        "\n",
        "for image_file in image_files:\n",
        "  file_name = os.path.splitext(image_file)[0]\n",
        "  class_name = re.sub('_\\d+', '', file_name)\n",
        "  if class_name == previous_class:\n",
        "    cnt += 1\n",
        "  else:\n",
        "    cnt = 1\n",
        "  if cnt <= 200:\n",
        "    for label in Labels:\n",
        "        if label == class_name:\n",
        "          cpath = os.path.join(train_dir, label)\n",
        "          image_path = os.path.join(image_dir, image_file)\n",
        "          shutil.copy(image_path, cpath)\n",
        "        else:\n",
        "          pass\n",
        "  else:\n",
        "    for label in Labels:\n",
        "        if label == class_name:\n",
        "          cpath = os.path.join(test_dir, label)\n",
        "          image_path = os.path.join(image_dir, image_file)\n",
        "          shutil.copy(image_path, cpath)\n",
        "        else:\n",
        "          pass\n",
        "  previous_class = class_name"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZ0tmzvMCYuG"
      },
      "source": [
        "# Data load and transform\n",
        "transform0 = transforms.Compose([\n",
        "                                transforms.Resize((512,512)), \n",
        "                                transforms.RandomHorizontalFlip(),\n",
        "                                transforms.ToTensor(),\n",
        "                                transforms.Normalize((0.5,0.5,0.5),\n",
        "                                                     (0.5,0.5,0.5))\n",
        "])\n",
        "\n",
        "transform1 = transforms.Compose([\n",
        "                                transforms.RandomCrop(224),\n",
        "                                transforms.RandomHorizontalFlip(),\n",
        "                                transforms.ToTensor(),\n",
        "                                transforms.Normalize((0.5,0.5,0.5),\n",
        "                                                     (0.5,0.5,0.5))\n",
        "])\n",
        "transform2 = transforms.Compose([\n",
        "                                transforms.RandomCrop(32, padding=4),\n",
        "                                transforms.RandomHorizontalFlip(),\n",
        "                                transforms.ToTensor(),\n",
        "                                transforms.Normalize((0.5,0.5,0.5),\n",
        "                                                     (0.5,0.5,0.5))\n",
        "])\n",
        "train_dataset = datasets.ImageFolder(root=train_dir, transform=transform0)\n",
        "test_dataset = datasets.ImageFolder(root=test_dir, transform=transform0)\n",
        "\n",
        "# Hyperparameter\n",
        "\n",
        "\"\"\"\n",
        "추가로 실험해봐야 할 부분\n",
        "1. epoch / batch size / lr / stepsize 조정\n",
        "2. crop 조정\n",
        "\"\"\"\n",
        "\n",
        "EPOCHS = 50       # 40, 150, 300\n",
        "BATCH_SIZE = 8   # 16, 64, 128\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    dataset = train_dataset,\n",
        "    batch_size = BATCH_SIZE\n",
        ")\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    dataset = test_dataset,\n",
        "    batch_size = BATCH_SIZE\n",
        ")"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        },
        "id": "Uo0OEsxpuAnr",
        "outputId": "15bebec0-1e92-4744-b995-56d778a89770"
      },
      "source": [
        "dataiter = iter(train_loader)\n",
        "images, labels = next(dataiter)\n",
        "img = utils.make_grid(images, padding=0)\n",
        "npimg = img.numpy()\n",
        "plt.figure(figsize = (10,7))\n",
        "plt.imshow(np.transpose(npimg,(1,2,0)))\n",
        "plt.show()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x504 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8VEjbj5_Uk5y",
        "outputId": "c0644358-ee3a-40f0-96da-cd3bb638cc8c"
      },
      "source": [
        "img.shape"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 512, 4096])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5vE9HvQZm_8r",
        "outputId": "297a4202-edcd-4c85-b1b4-129eba9a37af"
      },
      "source": [
        "print(len(Labels))\n",
        "\"\"\"\n",
        "다른 데이터셋으로 실험 시 반드시 num_classes를 label의 수로 변경\n",
        "이미지 사이즈 조정 시 출력계층 고치기 self.linear = nn.Linear(64*49, num_classes)\n",
        "\"\"\"\n",
        "Labels"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['b-1 lancer', 'f-35 lightning']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CTlmqBMN7ksj"
      },
      "source": [
        "class BasicBlock(nn.Module):\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3,\n",
        "                               stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, planes,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, num_classes=2):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_planes = 16\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(16)\n",
        "        self.layer1 = self._make_layer(16, 2, stride=1)\n",
        "        self.layer2 = self._make_layer(32, 2, stride=2)\n",
        "        self.layer3 = self._make_layer(64, 2, stride=2)\n",
        "        self.linear = nn.Linear(64*256, num_classes)\n",
        "\n",
        "    def _make_layer(self, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(BasicBlock(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = F.avg_pool2d(out, 8)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C0SUiS5yyJI_"
      },
      "source": [
        "model = ResNet().to(DEVICE)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1,\n",
        "                      momentum=0.9, weight_decay=0.005)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SrODV4UeJ6uZ",
        "outputId": "216f62a8-a6b6-4106-dca4-81f4734656d9"
      },
      "source": [
        "print(model)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ResNet(\n",
            "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "  (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (layer1): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential(\n",
            "        (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential(\n",
            "        (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "  )\n",
            "  (linear): Linear(in_features=16384, out_features=2, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FU17_tx8Nfqc"
      },
      "source": [
        "def train(model, train_loader, optimizer, epoch):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(DEVICE), target.to(DEVICE)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.cross_entropy(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Exbxq_vAYqoc"
      },
      "source": [
        "def evaluate(model, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(DEVICE), target.to(DEVICE)\n",
        "            output = model(data)\n",
        "\n",
        "            # 배치 오차를 합산\n",
        "            test_loss += F.cross_entropy(output, target,\n",
        "                                         reduction='mean').item()\n",
        "\n",
        "            # 가장 높은 값을 가진 인덱스가 바로 예측값\n",
        "            pred = output.max(1, keepdim=True)[1]\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    test_accuracy = 100. * correct / len(test_loader.dataset)\n",
        "    return test_loss, test_accuracy"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQ_PTUoYUHjm"
      },
      "source": [
        "import gc\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-UT6KQyBE5zm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d8c44aa-6df4-418c-f9d2-ceebaaa63ebf"
      },
      "source": [
        "for epoch in range(1, EPOCHS + 1):\n",
        "\n",
        "    train(model, train_loader, optimizer, epoch)\n",
        "    test_loss, test_accuracy = evaluate(model, test_loader)\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "    print('[{}] Test Loss: {:.4f}, Accuracy: {:.2f}%'.format(\n",
        "          epoch, test_loss, test_accuracy))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1] Test Loss: 13765153.0900, Accuracy: 13.00%\n",
            "[2] Test Loss: 445765.4475, Accuracy: 87.00%\n",
            "[3] Test Loss: 10377663233392.6406, Accuracy: 13.00%\n",
            "[4] Test Loss: 11882912885.7600, Accuracy: 13.00%\n",
            "[5] Test Loss: 490147302473014976.0000, Accuracy: 13.00%\n",
            "[6] Test Loss: nan, Accuracy: 87.00%\n",
            "[7] Test Loss: nan, Accuracy: 87.00%\n",
            "[8] Test Loss: nan, Accuracy: 87.00%\n",
            "[9] Test Loss: nan, Accuracy: 87.00%\n",
            "[10] Test Loss: nan, Accuracy: 87.00%\n",
            "[11] Test Loss: nan, Accuracy: 87.00%\n",
            "[12] Test Loss: nan, Accuracy: 87.00%\n",
            "[13] Test Loss: nan, Accuracy: 87.00%\n",
            "[14] Test Loss: nan, Accuracy: 87.00%\n",
            "[15] Test Loss: nan, Accuracy: 87.00%\n",
            "[16] Test Loss: nan, Accuracy: 87.00%\n",
            "[17] Test Loss: nan, Accuracy: 87.00%\n",
            "[18] Test Loss: nan, Accuracy: 87.00%\n",
            "[19] Test Loss: nan, Accuracy: 87.00%\n",
            "[20] Test Loss: nan, Accuracy: 87.00%\n",
            "[21] Test Loss: nan, Accuracy: 87.00%\n",
            "[22] Test Loss: nan, Accuracy: 87.00%\n",
            "[23] Test Loss: nan, Accuracy: 87.00%\n",
            "[24] Test Loss: nan, Accuracy: 87.00%\n",
            "[25] Test Loss: nan, Accuracy: 87.00%\n",
            "[26] Test Loss: nan, Accuracy: 87.00%\n",
            "[27] Test Loss: nan, Accuracy: 87.00%\n",
            "[28] Test Loss: nan, Accuracy: 87.00%\n",
            "[29] Test Loss: nan, Accuracy: 87.00%\n",
            "[30] Test Loss: nan, Accuracy: 87.00%\n",
            "[31] Test Loss: nan, Accuracy: 87.00%\n",
            "[32] Test Loss: nan, Accuracy: 87.00%\n",
            "[33] Test Loss: nan, Accuracy: 87.00%\n",
            "[34] Test Loss: nan, Accuracy: 87.00%\n",
            "[35] Test Loss: nan, Accuracy: 87.00%\n",
            "[36] Test Loss: nan, Accuracy: 87.00%\n",
            "[37] Test Loss: nan, Accuracy: 87.00%\n",
            "[38] Test Loss: nan, Accuracy: 87.00%\n",
            "[39] Test Loss: nan, Accuracy: 87.00%\n",
            "[40] Test Loss: nan, Accuracy: 87.00%\n",
            "[41] Test Loss: nan, Accuracy: 87.00%\n",
            "[42] Test Loss: nan, Accuracy: 87.00%\n",
            "[43] Test Loss: nan, Accuracy: 87.00%\n",
            "[44] Test Loss: nan, Accuracy: 87.00%\n",
            "[45] Test Loss: nan, Accuracy: 87.00%\n",
            "[46] Test Loss: nan, Accuracy: 87.00%\n",
            "[47] Test Loss: nan, Accuracy: 87.00%\n",
            "[48] Test Loss: nan, Accuracy: 87.00%\n",
            "[49] Test Loss: nan, Accuracy: 87.00%\n",
            "[50] Test Loss: nan, Accuracy: 87.00%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6IWBtfOqkNh"
      },
      "source": [
        "## 신경망 검사"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GosNa7uWp3Qf"
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "\n",
        "dataiter = iter(test_loader)\n",
        "images, labels = dataiter.next()"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5RUa3GwuiXO"
      },
      "source": [
        "## 가중치, 파라미터 확인"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DTTiSc4ezc9r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a0be814-fd5a-4702-e0b8-e4024b8b9556"
      },
      "source": [
        "print(\"Model's state dict:\")\n",
        "for param_tensor in model.state_dict():\n",
        "  print(param_tensor, '\\t', model.state_dict()[param_tensor].size())"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model's state dict:\n",
            "conv1.weight \t torch.Size([16, 3, 3, 3])\n",
            "bn1.weight \t torch.Size([16])\n",
            "bn1.bias \t torch.Size([16])\n",
            "bn1.running_mean \t torch.Size([16])\n",
            "bn1.running_var \t torch.Size([16])\n",
            "bn1.num_batches_tracked \t torch.Size([])\n",
            "layer1.0.conv1.weight \t torch.Size([16, 16, 3, 3])\n",
            "layer1.0.bn1.weight \t torch.Size([16])\n",
            "layer1.0.bn1.bias \t torch.Size([16])\n",
            "layer1.0.bn1.running_mean \t torch.Size([16])\n",
            "layer1.0.bn1.running_var \t torch.Size([16])\n",
            "layer1.0.bn1.num_batches_tracked \t torch.Size([])\n",
            "layer1.0.conv2.weight \t torch.Size([16, 16, 3, 3])\n",
            "layer1.0.bn2.weight \t torch.Size([16])\n",
            "layer1.0.bn2.bias \t torch.Size([16])\n",
            "layer1.0.bn2.running_mean \t torch.Size([16])\n",
            "layer1.0.bn2.running_var \t torch.Size([16])\n",
            "layer1.0.bn2.num_batches_tracked \t torch.Size([])\n",
            "layer1.1.conv1.weight \t torch.Size([16, 16, 3, 3])\n",
            "layer1.1.bn1.weight \t torch.Size([16])\n",
            "layer1.1.bn1.bias \t torch.Size([16])\n",
            "layer1.1.bn1.running_mean \t torch.Size([16])\n",
            "layer1.1.bn1.running_var \t torch.Size([16])\n",
            "layer1.1.bn1.num_batches_tracked \t torch.Size([])\n",
            "layer1.1.conv2.weight \t torch.Size([16, 16, 3, 3])\n",
            "layer1.1.bn2.weight \t torch.Size([16])\n",
            "layer1.1.bn2.bias \t torch.Size([16])\n",
            "layer1.1.bn2.running_mean \t torch.Size([16])\n",
            "layer1.1.bn2.running_var \t torch.Size([16])\n",
            "layer1.1.bn2.num_batches_tracked \t torch.Size([])\n",
            "layer2.0.conv1.weight \t torch.Size([32, 16, 3, 3])\n",
            "layer2.0.bn1.weight \t torch.Size([32])\n",
            "layer2.0.bn1.bias \t torch.Size([32])\n",
            "layer2.0.bn1.running_mean \t torch.Size([32])\n",
            "layer2.0.bn1.running_var \t torch.Size([32])\n",
            "layer2.0.bn1.num_batches_tracked \t torch.Size([])\n",
            "layer2.0.conv2.weight \t torch.Size([32, 32, 3, 3])\n",
            "layer2.0.bn2.weight \t torch.Size([32])\n",
            "layer2.0.bn2.bias \t torch.Size([32])\n",
            "layer2.0.bn2.running_mean \t torch.Size([32])\n",
            "layer2.0.bn2.running_var \t torch.Size([32])\n",
            "layer2.0.bn2.num_batches_tracked \t torch.Size([])\n",
            "layer2.0.shortcut.0.weight \t torch.Size([32, 16, 1, 1])\n",
            "layer2.0.shortcut.1.weight \t torch.Size([32])\n",
            "layer2.0.shortcut.1.bias \t torch.Size([32])\n",
            "layer2.0.shortcut.1.running_mean \t torch.Size([32])\n",
            "layer2.0.shortcut.1.running_var \t torch.Size([32])\n",
            "layer2.0.shortcut.1.num_batches_tracked \t torch.Size([])\n",
            "layer2.1.conv1.weight \t torch.Size([32, 32, 3, 3])\n",
            "layer2.1.bn1.weight \t torch.Size([32])\n",
            "layer2.1.bn1.bias \t torch.Size([32])\n",
            "layer2.1.bn1.running_mean \t torch.Size([32])\n",
            "layer2.1.bn1.running_var \t torch.Size([32])\n",
            "layer2.1.bn1.num_batches_tracked \t torch.Size([])\n",
            "layer2.1.conv2.weight \t torch.Size([32, 32, 3, 3])\n",
            "layer2.1.bn2.weight \t torch.Size([32])\n",
            "layer2.1.bn2.bias \t torch.Size([32])\n",
            "layer2.1.bn2.running_mean \t torch.Size([32])\n",
            "layer2.1.bn2.running_var \t torch.Size([32])\n",
            "layer2.1.bn2.num_batches_tracked \t torch.Size([])\n",
            "layer3.0.conv1.weight \t torch.Size([64, 32, 3, 3])\n",
            "layer3.0.bn1.weight \t torch.Size([64])\n",
            "layer3.0.bn1.bias \t torch.Size([64])\n",
            "layer3.0.bn1.running_mean \t torch.Size([64])\n",
            "layer3.0.bn1.running_var \t torch.Size([64])\n",
            "layer3.0.bn1.num_batches_tracked \t torch.Size([])\n",
            "layer3.0.conv2.weight \t torch.Size([64, 64, 3, 3])\n",
            "layer3.0.bn2.weight \t torch.Size([64])\n",
            "layer3.0.bn2.bias \t torch.Size([64])\n",
            "layer3.0.bn2.running_mean \t torch.Size([64])\n",
            "layer3.0.bn2.running_var \t torch.Size([64])\n",
            "layer3.0.bn2.num_batches_tracked \t torch.Size([])\n",
            "layer3.0.shortcut.0.weight \t torch.Size([64, 32, 1, 1])\n",
            "layer3.0.shortcut.1.weight \t torch.Size([64])\n",
            "layer3.0.shortcut.1.bias \t torch.Size([64])\n",
            "layer3.0.shortcut.1.running_mean \t torch.Size([64])\n",
            "layer3.0.shortcut.1.running_var \t torch.Size([64])\n",
            "layer3.0.shortcut.1.num_batches_tracked \t torch.Size([])\n",
            "layer3.1.conv1.weight \t torch.Size([64, 64, 3, 3])\n",
            "layer3.1.bn1.weight \t torch.Size([64])\n",
            "layer3.1.bn1.bias \t torch.Size([64])\n",
            "layer3.1.bn1.running_mean \t torch.Size([64])\n",
            "layer3.1.bn1.running_var \t torch.Size([64])\n",
            "layer3.1.bn1.num_batches_tracked \t torch.Size([])\n",
            "layer3.1.conv2.weight \t torch.Size([64, 64, 3, 3])\n",
            "layer3.1.bn2.weight \t torch.Size([64])\n",
            "layer3.1.bn2.bias \t torch.Size([64])\n",
            "layer3.1.bn2.running_mean \t torch.Size([64])\n",
            "layer3.1.bn2.running_var \t torch.Size([64])\n",
            "layer3.1.bn2.num_batches_tracked \t torch.Size([])\n",
            "linear.weight \t torch.Size([2, 16384])\n",
            "linear.bias \t torch.Size([2])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uootomSzl6FN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f6baa18-32c8-41fa-ea36-c5aa77767ef5"
      },
      "source": [
        "print(\"Optimizer's state_dict:\")\n",
        "for var_name in optimizer.state_dict():\n",
        "  print(var_name, '\\t', optimizer.state_dict()[var_name])"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Optimizer's state_dict:\n",
            "state \t {0: {'momentum_buffer': tensor([[[[-2.0367e+02, -1.9317e+02, -2.0058e+02],\n",
            "          [-2.1546e+02, -2.1226e+02, -2.2673e+02],\n",
            "          [-2.4980e+02, -2.5555e+02, -2.7632e+02]],\n",
            "\n",
            "         [[-2.0367e+02, -1.9317e+02, -2.0058e+02],\n",
            "          [-2.1546e+02, -2.1226e+02, -2.2673e+02],\n",
            "          [-2.4980e+02, -2.5555e+02, -2.7632e+02]],\n",
            "\n",
            "         [[-2.0367e+02, -1.9317e+02, -2.0058e+02],\n",
            "          [-2.1546e+02, -2.1226e+02, -2.2673e+02],\n",
            "          [-2.4980e+02, -2.5555e+02, -2.7632e+02]]],\n",
            "\n",
            "\n",
            "        [[[-1.0575e-02, -2.6706e-02, -2.5344e-02],\n",
            "          [ 2.8083e-02,  1.1068e-02,  9.9231e-03],\n",
            "          [ 3.0961e-02,  1.3636e-02,  1.2159e-02]],\n",
            "\n",
            "         [[-1.0447e-02, -2.7135e-02, -2.5825e-02],\n",
            "          [ 2.7791e-02,  1.0731e-02,  8.9658e-03],\n",
            "          [ 3.1241e-02,  1.4064e-02,  1.2071e-02]],\n",
            "\n",
            "         [[-1.1018e-02, -2.6248e-02, -2.5733e-02],\n",
            "          [ 2.7696e-02,  1.1208e-02,  9.0837e-03],\n",
            "          [ 3.1681e-02,  1.3804e-02,  1.1735e-02]]],\n",
            "\n",
            "\n",
            "        [[[-3.2049e-01, -3.2433e-01, -3.9381e-01],\n",
            "          [-4.8517e-01, -4.9056e-01, -5.6316e-01],\n",
            "          [-3.6628e-01, -3.8401e-01, -4.5356e-01]],\n",
            "\n",
            "         [[-3.2132e-01, -3.2496e-01, -3.9299e-01],\n",
            "          [-4.8554e-01, -4.9048e-01, -5.6321e-01],\n",
            "          [-3.6697e-01, -3.8479e-01, -4.5394e-01]],\n",
            "\n",
            "         [[-3.2104e-01, -3.2441e-01, -3.9341e-01],\n",
            "          [-4.8480e-01, -4.9086e-01, -5.6348e-01],\n",
            "          [-3.6658e-01, -3.8461e-01, -4.5347e-01]]],\n",
            "\n",
            "\n",
            "        [[[ 1.2826e+01,  1.3411e+01,  1.3717e+01],\n",
            "          [ 1.2030e+01,  1.2670e+01,  1.3104e+01],\n",
            "          [ 1.0702e+01,  1.1333e+01,  1.1844e+01]],\n",
            "\n",
            "         [[ 1.2827e+01,  1.3412e+01,  1.3718e+01],\n",
            "          [ 1.2030e+01,  1.2670e+01,  1.3103e+01],\n",
            "          [ 1.0703e+01,  1.1333e+01,  1.1844e+01]],\n",
            "\n",
            "         [[ 1.2827e+01,  1.3411e+01,  1.3717e+01],\n",
            "          [ 1.2030e+01,  1.2670e+01,  1.3103e+01],\n",
            "          [ 1.0703e+01,  1.1332e+01,  1.1844e+01]]],\n",
            "\n",
            "\n",
            "        [[[-1.6747e-01,  6.2674e-02,  1.5375e-01],\n",
            "          [-3.2828e-01, -1.0602e-01, -1.1215e-02],\n",
            "          [-6.2032e-01, -4.2358e-01, -3.3352e-01]],\n",
            "\n",
            "         [[-1.6734e-01,  6.2762e-02,  1.5416e-01],\n",
            "          [-3.2746e-01, -1.0565e-01, -1.0979e-02],\n",
            "          [-6.1994e-01, -4.2333e-01, -3.3419e-01]],\n",
            "\n",
            "         [[-1.6710e-01,  6.2776e-02,  1.5387e-01],\n",
            "          [-3.2757e-01, -1.0553e-01, -1.1354e-02],\n",
            "          [-6.2097e-01, -4.2288e-01, -3.3403e-01]]],\n",
            "\n",
            "\n",
            "        [[[        nan,         nan,         nan],\n",
            "          [        nan,         nan,         nan],\n",
            "          [        nan,         nan,         nan]],\n",
            "\n",
            "         [[        nan,         nan,         nan],\n",
            "          [        nan,         nan,         nan],\n",
            "          [        nan,         nan,         nan]],\n",
            "\n",
            "         [[        nan,         nan,         nan],\n",
            "          [        nan,         nan,         nan],\n",
            "          [        nan,         nan,         nan]]],\n",
            "\n",
            "\n",
            "        [[[ 9.6179e+01,  9.5755e+01,  9.2963e+01],\n",
            "          [ 9.2349e+01,  9.0132e+01,  8.6044e+01],\n",
            "          [ 8.4743e+01,  8.1029e+01,  7.6243e+01]],\n",
            "\n",
            "         [[ 9.6180e+01,  9.5755e+01,  9.2963e+01],\n",
            "          [ 9.2349e+01,  9.0132e+01,  8.6044e+01],\n",
            "          [ 8.4743e+01,  8.1028e+01,  7.6244e+01]],\n",
            "\n",
            "         [[ 9.6179e+01,  9.5755e+01,  9.2964e+01],\n",
            "          [ 9.2350e+01,  9.0132e+01,  8.6045e+01],\n",
            "          [ 8.4744e+01,  8.1029e+01,  7.6244e+01]]],\n",
            "\n",
            "\n",
            "        [[[-3.3456e-02,  3.4027e-01,  4.6954e-01],\n",
            "          [-3.9781e-02,  3.2552e-01,  4.5902e-01],\n",
            "          [-3.3040e-01,  1.8412e-03,  1.2491e-01]],\n",
            "\n",
            "         [[-3.3743e-02,  3.4050e-01,  4.6904e-01],\n",
            "          [-3.9696e-02,  3.2614e-01,  4.5842e-01],\n",
            "          [-3.3031e-01,  2.6327e-03,  1.2518e-01]],\n",
            "\n",
            "         [[-3.3723e-02,  3.3970e-01,  4.6984e-01],\n",
            "          [-4.0116e-02,  3.2635e-01,  4.5822e-01],\n",
            "          [-3.3026e-01,  2.2613e-03,  1.2430e-01]]],\n",
            "\n",
            "\n",
            "        [[[-5.1043e-02, -5.8893e-03,  1.6130e-03],\n",
            "          [-4.9949e-02, -2.5771e-03,  6.4222e-03],\n",
            "          [-7.2012e-02, -2.3575e-02, -1.3506e-02]],\n",
            "\n",
            "         [[-5.0301e-02, -6.3884e-03,  1.3062e-03],\n",
            "          [-4.9477e-02, -2.5501e-03,  5.9189e-03],\n",
            "          [-7.1491e-02, -2.3386e-02, -1.2559e-02]],\n",
            "\n",
            "         [[-5.0124e-02, -6.2389e-03,  1.9711e-03],\n",
            "          [-4.9253e-02, -3.3476e-03,  5.8422e-03],\n",
            "          [-7.1647e-02, -2.2957e-02, -1.2651e-02]]],\n",
            "\n",
            "\n",
            "        [[[-1.9226e-01, -8.6192e-02, -6.6746e-02],\n",
            "          [-1.0837e-01,  2.7146e-03,  2.8630e-02],\n",
            "          [-1.3402e-01, -3.0119e-02,  1.0756e-02]],\n",
            "\n",
            "         [[-1.9151e-01, -8.6578e-02, -6.6884e-02],\n",
            "          [-1.0789e-01,  2.1145e-03,  2.7938e-02],\n",
            "          [-1.3387e-01, -2.9890e-02,  1.0886e-02]],\n",
            "\n",
            "         [[-1.9219e-01, -8.6987e-02, -6.6254e-02],\n",
            "          [-1.0795e-01,  2.2275e-03,  2.8845e-02],\n",
            "          [-1.3332e-01, -3.0211e-02,  1.1021e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 7.2294e-04,  1.7861e-03,  3.2517e-03],\n",
            "          [-4.2748e-03, -3.3023e-03, -3.0064e-03],\n",
            "          [-1.5675e-02, -1.5094e-02, -1.5059e-02]],\n",
            "\n",
            "         [[ 3.0069e-04,  1.8905e-03,  2.9568e-03],\n",
            "          [-4.3595e-03, -3.2895e-03, -2.8841e-03],\n",
            "          [-1.6184e-02, -1.5199e-02, -1.5003e-02]],\n",
            "\n",
            "         [[ 1.0427e-03,  1.6178e-03,  3.6946e-03],\n",
            "          [-4.5749e-03, -3.2034e-03, -3.2891e-03],\n",
            "          [-1.5531e-02, -1.5207e-02, -1.5042e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 1.0609e+05,  5.0840e+05,  6.3037e+05],\n",
            "          [ 1.3950e+06,  1.6943e+06,  1.7680e+06],\n",
            "          [-3.9310e+05, -2.1517e+05, -2.5879e+05]],\n",
            "\n",
            "         [[ 1.0609e+05,  5.0840e+05,  6.3037e+05],\n",
            "          [ 1.3950e+06,  1.6943e+06,  1.7680e+06],\n",
            "          [-3.9310e+05, -2.1517e+05, -2.5879e+05]],\n",
            "\n",
            "         [[ 1.0609e+05,  5.0840e+05,  6.3037e+05],\n",
            "          [ 1.3950e+06,  1.6943e+06,  1.7680e+06],\n",
            "          [-3.9310e+05, -2.1517e+05, -2.5879e+05]]],\n",
            "\n",
            "\n",
            "        [[[        nan,         nan,         nan],\n",
            "          [        nan,         nan,         nan],\n",
            "          [        nan,         nan,         nan]],\n",
            "\n",
            "         [[        nan,         nan,         nan],\n",
            "          [        nan,         nan,         nan],\n",
            "          [        nan,         nan,         nan]],\n",
            "\n",
            "         [[        nan,         nan,         nan],\n",
            "          [        nan,         nan,         nan],\n",
            "          [        nan,         nan,         nan]]],\n",
            "\n",
            "\n",
            "        [[[ 8.9948e-01,  1.0971e+00,  1.4439e+00],\n",
            "          [ 1.5405e+00,  1.7964e+00,  2.2850e+00],\n",
            "          [ 2.6669e+00,  3.0194e+00,  3.6969e+00]],\n",
            "\n",
            "         [[ 8.9888e-01,  1.0973e+00,  1.4437e+00],\n",
            "          [ 1.5399e+00,  1.7971e+00,  2.2848e+00],\n",
            "          [ 2.6661e+00,  3.0198e+00,  3.6966e+00]],\n",
            "\n",
            "         [[ 8.9931e-01,  1.0970e+00,  1.4433e+00],\n",
            "          [ 1.5404e+00,  1.7963e+00,  2.2849e+00],\n",
            "          [ 2.6661e+00,  3.0199e+00,  3.6968e+00]]],\n",
            "\n",
            "\n",
            "        [[[ 2.1154e+03,  2.0443e+03,  1.8925e+03],\n",
            "          [ 3.6184e+03,  3.2502e+03,  2.9016e+03],\n",
            "          [ 7.8273e+03,  7.3956e+03,  6.8001e+03]],\n",
            "\n",
            "         [[ 2.1154e+03,  2.0443e+03,  1.8925e+03],\n",
            "          [ 3.6184e+03,  3.2502e+03,  2.9016e+03],\n",
            "          [ 7.8273e+03,  7.3956e+03,  6.8001e+03]],\n",
            "\n",
            "         [[ 2.1155e+03,  2.0443e+03,  1.8925e+03],\n",
            "          [ 3.6184e+03,  3.2502e+03,  2.9016e+03],\n",
            "          [ 7.8273e+03,  7.3956e+03,  6.8001e+03]]],\n",
            "\n",
            "\n",
            "        [[[ 2.9258e+01,  3.1342e+01,  3.3440e+01],\n",
            "          [ 3.2838e+01,  3.6337e+01,  3.8359e+01],\n",
            "          [ 3.4938e+01,  4.0250e+01,  4.2207e+01]],\n",
            "\n",
            "         [[ 2.9258e+01,  3.1343e+01,  3.3440e+01],\n",
            "          [ 3.2839e+01,  3.6336e+01,  3.8359e+01],\n",
            "          [ 3.4938e+01,  4.0249e+01,  4.2206e+01]],\n",
            "\n",
            "         [[ 2.9258e+01,  3.1343e+01,  3.3440e+01],\n",
            "          [ 3.2838e+01,  3.6337e+01,  3.8360e+01],\n",
            "          [ 3.4938e+01,  4.0250e+01,  4.2206e+01]]]], device='cuda:0')}, 1: {'momentum_buffer': tensor([-1.0024e+02,  1.8442e-03,  4.9667e-02,  1.0306e+00, -3.9279e-02,\n",
            "                nan,  2.0563e+00,  1.4138e-01, -1.7039e-01, -1.7939e-01,\n",
            "        -2.8500e-01,  9.4028e+05,         nan, -1.0694e-01, -4.5295e+01,\n",
            "         1.4582e+02], device='cuda:0')}, 2: {'momentum_buffer': tensor([-6.3704e+01, -4.5403e-02, -4.8873e-01, -8.5396e-01, -2.1898e-01,\n",
            "                nan, -5.5802e+00, -6.0029e-01, -2.1988e-01, -4.2943e-01,\n",
            "        -5.7830e-01, -1.9657e+06,         nan, -1.8529e+00, -2.6276e+03,\n",
            "        -1.0546e+02], device='cuda:0')}, 3: {'momentum_buffer': tensor([[[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]]], device='cuda:0')}, 4: {'momentum_buffer': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
            "       device='cuda:0')}, 5: {'momentum_buffer': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
            "       device='cuda:0')}, 6: {'momentum_buffer': tensor([[[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]]], device='cuda:0')}, 7: {'momentum_buffer': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
            "       device='cuda:0')}, 8: {'momentum_buffer': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
            "       device='cuda:0')}, 9: {'momentum_buffer': tensor([[[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]]], device='cuda:0')}, 10: {'momentum_buffer': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
            "       device='cuda:0')}, 11: {'momentum_buffer': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
            "       device='cuda:0')}, 12: {'momentum_buffer': tensor([[[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]]], device='cuda:0')}, 13: {'momentum_buffer': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
            "       device='cuda:0')}, 14: {'momentum_buffer': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
            "       device='cuda:0')}, 15: {'momentum_buffer': tensor([[[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]]], device='cuda:0')}, 16: {'momentum_buffer': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0')}, 17: {'momentum_buffer': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0')}, 18: {'momentum_buffer': tensor([[[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]]], device='cuda:0')}, 19: {'momentum_buffer': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0')}, 20: {'momentum_buffer': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0')}, 21: {'momentum_buffer': tensor([[[[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]]],\n",
            "\n",
            "\n",
            "        [[[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]]],\n",
            "\n",
            "\n",
            "        [[[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]]],\n",
            "\n",
            "\n",
            "        [[[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]]],\n",
            "\n",
            "\n",
            "        [[[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]]],\n",
            "\n",
            "\n",
            "        [[[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]]],\n",
            "\n",
            "\n",
            "        [[[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]]],\n",
            "\n",
            "\n",
            "        [[[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]]],\n",
            "\n",
            "\n",
            "        [[[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]]],\n",
            "\n",
            "\n",
            "        [[[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]]],\n",
            "\n",
            "\n",
            "        [[[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]]],\n",
            "\n",
            "\n",
            "        [[[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]]],\n",
            "\n",
            "\n",
            "        [[[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]]],\n",
            "\n",
            "\n",
            "        [[[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]]],\n",
            "\n",
            "\n",
            "        [[[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]]],\n",
            "\n",
            "\n",
            "        [[[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]]],\n",
            "\n",
            "\n",
            "        [[[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]]],\n",
            "\n",
            "\n",
            "        [[[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]]],\n",
            "\n",
            "\n",
            "        [[[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]]],\n",
            "\n",
            "\n",
            "        [[[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]]],\n",
            "\n",
            "\n",
            "        [[[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]]],\n",
            "\n",
            "\n",
            "        [[[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]]],\n",
            "\n",
            "\n",
            "        [[[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]]],\n",
            "\n",
            "\n",
            "        [[[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]]],\n",
            "\n",
            "\n",
            "        [[[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]]],\n",
            "\n",
            "\n",
            "        [[[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]]],\n",
            "\n",
            "\n",
            "        [[[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]]],\n",
            "\n",
            "\n",
            "        [[[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]]],\n",
            "\n",
            "\n",
            "        [[[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]]],\n",
            "\n",
            "\n",
            "        [[[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]]],\n",
            "\n",
            "\n",
            "        [[[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]]],\n",
            "\n",
            "\n",
            "        [[[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]]]], device='cuda:0')}, 22: {'momentum_buffer': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0')}, 23: {'momentum_buffer': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0')}, 24: {'momentum_buffer': tensor([[[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]]], device='cuda:0')}, 25: {'momentum_buffer': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0')}, 26: {'momentum_buffer': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0')}, 27: {'momentum_buffer': tensor([[[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]]], device='cuda:0')}, 28: {'momentum_buffer': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0')}, 29: {'momentum_buffer': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0')}, 30: {'momentum_buffer': tensor([[[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]]], device='cuda:0')}, 31: {'momentum_buffer': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
            "       device='cuda:0')}, 32: {'momentum_buffer': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
            "       device='cuda:0')}, 33: {'momentum_buffer': tensor([[[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]]], device='cuda:0')}, 34: {'momentum_buffer': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
            "       device='cuda:0')}, 35: {'momentum_buffer': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
            "       device='cuda:0')}, 36: {'momentum_buffer': tensor([[[[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]]],\n",
            "\n",
            "\n",
            "        [[[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]]],\n",
            "\n",
            "\n",
            "        [[[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]]],\n",
            "\n",
            "\n",
            "        [[[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]]],\n",
            "\n",
            "\n",
            "        [[[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]],\n",
            "\n",
            "         [[nan]]]], device='cuda:0')}, 37: {'momentum_buffer': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
            "       device='cuda:0')}, 38: {'momentum_buffer': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
            "       device='cuda:0')}, 39: {'momentum_buffer': tensor([[[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]]], device='cuda:0')}, 40: {'momentum_buffer': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
            "       device='cuda:0')}, 41: {'momentum_buffer': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
            "       device='cuda:0')}, 42: {'momentum_buffer': tensor([[[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]],\n",
            "\n",
            "\n",
            "        [[[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]],\n",
            "\n",
            "         [[nan, nan, nan],\n",
            "          [nan, nan, nan],\n",
            "          [nan, nan, nan]]]], device='cuda:0')}, 43: {'momentum_buffer': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
            "       device='cuda:0')}, 44: {'momentum_buffer': tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
            "       device='cuda:0')}, 45: {'momentum_buffer': tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
            "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0')}, 46: {'momentum_buffer': tensor([nan, nan], device='cuda:0')}}\n",
            "param_groups \t [{'lr': 1.0000000000000004e-06, 'momentum': 0.9, 'dampening': 0, 'weight_decay': 0.005, 'nesterov': False, 'initial_lr': 0.1, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46]}]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6AgPoA0u18d"
      },
      "source": [
        "## 모델 저장"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hd32m4Gy6Ua0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "307e8006-a15e-4fb8-a0b3-8c00efc1b795"
      },
      "source": [
        "# save model\n",
        "\n",
        "PATH = './model_v0.04.pth'\n",
        "torch.save(model.state_dict(), PATH)\n",
        "\"\"\"\n",
        "불러올 땐\n",
        "model = TheModelClass(*args, **kwargs)\n",
        "model.load_state_dict(torch.load(PATH))\n",
        "model.eval()\n",
        "\"\"\""
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n불러올 땐\\nmodel = TheModelClass(*args, **kwargs)\\nmodel.load_state_dict(torch.load(PATH))\\nmodel.eval()\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JdR4jNelXBNR"
      },
      "source": [
        "# construct model on cuda if available\n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "\n",
        "class CNNClassifier(nn.Module):\n",
        "    \n",
        "    def __init__(self):\n",
        "        # 항상 torch.nn.Module을 상속받고 시작\n",
        "        super(CNNClassifier, self).__init__()\n",
        "        conv1 = nn.Conv2d(3, 512, 3, stride=1) # in channels, out_channels, kernel_size, stride\n",
        "        # activation ReLU\n",
        "        pool1 = nn.MaxPool2d(2) # 6@12*12\n",
        "        conv2 = nn.Conv2d(512, 1024, 5, stride=1) # in channels, out_channels, kernel_size, stride\n",
        "        # activation ReLU\n",
        "        pool2 = nn.MaxPool2d(2) # 16@4*4\n",
        "        \n",
        "        self.conv_module = nn.Sequential(\n",
        "            conv1,\n",
        "            nn.ReLU(),\n",
        "            pool1,\n",
        "            conv2,\n",
        "            nn.ReLU(),\n",
        "            pool2\n",
        "        )\n",
        "        \n",
        "        fc1 = nn.Linear(32768, 1024)\n",
        "        # activation ReLU\n",
        "        fc2 = nn.Linear(1024, 64)\n",
        "        # activation ReLU\n",
        "        fc3 = nn.Linear(64, 10)\n",
        "\n",
        "        self.fc_module = nn.Sequential(\n",
        "            fc1,\n",
        "            nn.ReLU(),\n",
        "            fc2,\n",
        "            nn.ReLU(),\n",
        "            fc3\n",
        "        )\n",
        "        \n",
        "        # gpu로 할당\n",
        "        if use_cuda:\n",
        "            self.conv_module = self.conv_module.cuda()\n",
        "            self.fc_module = self.fc_module.cuda()\n",
        "        \n",
        "    def forward(self, x):\n",
        "        out = self.conv_module(x) # @16*4*4\n",
        "        # make linear\n",
        "        dim = 1\n",
        "        for d in out.size()[1:]: #16, 4, 4\n",
        "            dim = dim * d\n",
        "        out = out.view(-1, dim)\n",
        "        out = self.fc_module(out)\n",
        "        return F.softmax(out, dim=1)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6lvBD-IjY_1y"
      },
      "source": [
        "model2 = CNNClassifier()"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZ6eAxMpZBhj"
      },
      "source": [
        "learning_rate = 0.01\n",
        "optimizer = optim.Adam(model2.parameters(), lr=learning_rate)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B456Y7f04AHY"
      },
      "source": [
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Q6JzHk38Oar",
        "outputId": "efbd47e6-27d2-4bad-bbff-14aa1e097007",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 361
        }
      },
      "source": [
        "# create figure for plotting\n",
        "import itertools\n",
        "row_num = 2\n",
        "col_num = 4\n",
        "fig, ax = plt.subplots(row_num, col_num, figsize=(6,6))\n",
        "for i, j in itertools.product(range(row_num), range(col_num)):\n",
        "    ax[i,j].get_xaxis().set_visible(False)\n",
        "    ax[i,j].get_yaxis().set_visible(False) \n",
        "    \n",
        "trn_loss_list = []\n",
        "val_loss_list = []\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x432 with 8 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "15O5Z6R-9uwm"
      },
      "source": [
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VvrBAEF1ZB2O",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "outputId": "2153dfa3-b66b-49e9-e14e-9c6757209ead"
      },
      "source": [
        "for epoch in range(EPOCHS):\n",
        "    trn_loss = 0.0\n",
        "    for i, data in enumerate(train_loader):\n",
        "        x, label = data\n",
        "        if use_cuda:\n",
        "            x = x.cuda()\n",
        "            label = label.cuda()\n",
        "        # grad init\n",
        "        optimizer.zero_grad()\n",
        "        # forward propagation\n",
        "        model_output = model2(x)\n",
        "        # calculate loss\n",
        "        loss = criterion(model_output, label)\n",
        "        # back propagation \n",
        "        loss.backward()\n",
        "        # weight update\n",
        "        optimizer.step()\n",
        "        \n",
        "        # trn_loss summary\n",
        "        train_loss += loss.item()\n",
        "        # del (memory issue)\n",
        "        del loss\n",
        "        del model_output\n",
        "        \n",
        "        # 학습과정 출력\n",
        "        if (i+1) % 10 == 0: # every 10 or 100 mini-batches\n",
        "            with torch.no_grad(): # very very very very important!!!\n",
        "                test_loss = 0.0\n",
        "                for j, val in enumerate(test_loader):\n",
        "                    test_x, test_label = test\n",
        "                    if use_cuda:\n",
        "                        test_x = test_x.cuda()\n",
        "                        test_label = test_label.cuda()\n",
        "                    test_output = cnn(val_x)\n",
        "                    t_loss = criterion(test_output, test_label)\n",
        "                    test_loss += t_loss\n",
        "\n",
        "            # draw last val dataset\n",
        "            for k in range(row_num*col_num):\n",
        "                ii = k//col_num\n",
        "                jj = k%col_num\n",
        "                ax[ii,jj].cla() # clear the current axis\n",
        "                ax[ii,jj].imshow(val_x[k,:].data.cpu().numpy().reshape(28,28), cmap='Greys')\n",
        "            \n",
        "            display.clear_output(wait=True)\n",
        "            display.display(plt.gcf()) # get a reference to a current figure\n",
        "                \n",
        "            print(\"label: {}\".format(val_label[:row_num*col_num]))\n",
        "            print(\"prediction: {}\".format(val_output.argmax(dim=1)[:row_num*col_num]))\n",
        "            del val_output\n",
        "            del v_loss\n",
        "            \n",
        "            print(\"epoch: {}/{} | step: {}/{} | trn loss: {:.4f} | val loss: {:.4f}\".format(\n",
        "                epoch+1, num_epochs, i+1, num_batches, trn_loss / 100, val_loss / len(val_loader)\n",
        "            ))            \n",
        "            \n",
        "            train_loss_list.append(train_loss/100)\n",
        "            test_loss_list.append(test_loss/len(test_loader))\n",
        "            train_loss = 0.0"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-258895eb961f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;31m# forward propagation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mmodel_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0;31m# calculate loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-28-55f9f73d820b>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# @16*4*4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m         \u001b[0;31m# make linear\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/activation.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mrelu\u001b[0;34m(input, inplace)\u001b[0m\n\u001b[1;32m   1296\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1297\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1298\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1299\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 1.92 GiB (GPU 0; 14.76 GiB total capacity; 13.06 GiB already allocated; 623.75 MiB free; 13.07 GiB reserved in total by PyTorch)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJ0EptHBZB-j"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nXOMWNpOZCFO"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s5kS454XZCK7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g0g7DICNZCQb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZkJ35KctZCVz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "14OyExMFXJqU"
      },
      "source": [
        "model2 = ResNet2().to(DEVICE)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1,\n",
        "                      momentum=0.9, weight_decay=0.005)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}